---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 16, 2024 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: today
format:
  # pdf:
  #   documentclass: article
  #   toc: true
  #   toc-depth: 2
  #   keep-tex: true
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 9.7.1 (10pts)

![](images/clipboard-2052603752.png)

------------------------------------------------------------------------

Solution:

\(a\)

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Generate random points
set.seed(123)  # For reproducibility
points <- data.frame(
  X1 = runif(100, min = -5, max = 5),  # Random X1 values
  X2 = runif(100, min = -5, max = 5)   # Random X2 values
)

# Define hyperplane equation
hyperplane_1 <- function(X1) { 1 + 3*X1 }  # Equation for hyperplane 1

# Calculate the corresponding values of X2
points$hyperplane_1_result <- 1 + 3*points$X1 - points$X2  # Evaluate the hyperplane equation

# Assign category labels for each point based on hyperplane condition
points$region_1 <- ifelse(points$hyperplane_1_result > 0, "1 + 3X1 - X2 > 0", "1 + 3X1 - X2 < 0")

# Combine the region labels to distinguish the points based on the regions
points$final_region <- paste(points$region_1)

# Create the plot
ggplot(points) +
  # Plot random points with color based on region
  geom_point(aes(x = X1, y = X2, color = final_region), size = 3) +
  # Plot the hyperplane line
  geom_abline(intercept = 1, slope = 3, color = "yellow", size = 1) +  # Hyperplane: 1 + 3X1 - X2 = 0
  labs(title = "Hyperplane Plot with Random Points", x = "X1", y = "X2") +  # Set plot title and axis labels
  theme_minimal() +  # Use minimal theme for cleaner plot
  theme(legend.position = "top") +  # Position the legend at the top
  scale_color_manual(values = c("blue", "red"))  # Set colors for the regions



```

\(b\)

```{r}
set.seed(123)  # For reproducibility
points <- data.frame(
  X1 = runif(100, min = -5, max = 5),  # Random X1 values
  X2 = runif(100, min = -5, max = 5)   # Random X2 values
)

# Define hyperplane equations
hyperplane_1 <- function(X1) { 1 + 3*X1 }
hyperplane_2 <- function(X1) { (-2 + X1) / 2 }

# Calculate corresponding X2 values
points$hyperplane_1_result <- 1 + 3*points$X1 - points$X2
points$hyperplane_2_result <- -2 + points$X1 + 2*points$X2

# Assign category labels for each point
points$region_1 <- ifelse(points$hyperplane_1_result > 0, "1 + 3X1 - X2 > 0", "1 + 3X1 - X2 < 0")
points$region_2 <- ifelse(points$hyperplane_2_result > 0, "-2 + X1 + 2X2 > 0", "-2 + X1 + 2X2 < 0")

# Combine both labels to distinguish different regions of the points
points$final_region <- paste(points$region_1, points$region_2)

# Plot
ggplot(points) +
  geom_point(aes(x = X1, y = X2, color = final_region), size = 3) +  # Random points with color based on regions
  geom_abline(intercept = 1, slope = 3, color = "yellow", size = 1, show.legend = FALSE) +  # Hyperplane 1 without legend
  geom_abline(intercept = -2, slope = -1/2, color = "green", size = 1, show.legend = FALSE) +  # Hyperplane 2 without legend
  labs(title = "Hyperplane Plot with Random Points", x = "X1", y = "X2") +
  scale_color_manual(values = c("blue", "red", "purple", "gray")) +  # Set 4 colors for the regions
  theme_minimal() + 
  theme(
    legend.position = "right", 
    legend.title = element_blank(),
    legend.text = element_text(size = 7),  # Smaller text for labels
    legend.key.size = unit(0.4, "cm"),  # Adjust legend key size for better spacing
    legend.direction = "vertical",  # Arrange labels vertically
    legend.box = "vertical",  # Stack the labels vertically
    plot.margin = margin(10, 10, 10, 10),  # Adjust margins to give more space
    aspect.ratio = 0.8  # Increase the size of the plot
  )


```

## ISL Exercise 9.7.2 (10pts)

![](images/clipboard-3787525584.png)

------------------------------------------------------------------------

Solution:

\(a\)

```{r}
# Define the circle equation
circle_eq <- function(X1) {
  sqrt(4 - (X1 + 1)^2) + 2  # Solve for X2
}

# Create X1 values and calculate corresponding X2 values
X1_values <- seq(-3, 1, by = 0.1)
X2_values_positive <- sapply(X1_values, function(X1) circle_eq(X1))
X2_values_negative <- sapply(X1_values, function(X1) -circle_eq(X1) + 4)

# Create a data frame
circle_data <- data.frame(X1 = rep(X1_values, 2),
                          X2 = c(X2_values_positive, X2_values_negative),
                          Type = rep(c("Upper half", "Lower half"), each = length(X1_values)))

# Plot the circle
ggplot(circle_data, aes(x = X1, y = X2, color = Type)) +
  geom_line() +
  labs(title = "Plot of the Circle (1 + X1)^2 + (2 - X2)^2 = 4", x = "X1", y = "X2") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red")) +
  coord_fixed(ratio = 1) 


```

\(b\)

```{r}
# Define random points to classify as inside or outside the circle
set.seed(213)  # For reproducibility
points <- data.frame(
  X1 = runif(200, min = -4, max = 2),  # Random X1 values
  X2 = runif(200, min = -1, max = 5)   # Random X2 values
)

# Calculate the circle equation for each point
points$circle_equation <- (1 + points$X1)^2 + (2 - points$X2)^2

# Assign categories based on whether points are inside or outside the circle
points$region <- ifelse(points$circle_equation <= 4, "Inside the Circle", "Outside the Circle")

# Plot the circle and points with different colors for the regions
ggplot() +
  geom_line(data = circle_data, aes(x = X1, y = X2, color = Type), size = 1) +  # Circle
  geom_point(data = points, aes(x = X1, y = X2, color = region), size = 2) +  # Points inside and outside the circle
  labs(title = "Plot of the Circle (1 + X1)^2 + (2 - X2)^2 = 4", x = "X1", y = "X2") +
  scale_color_manual(values = c("yellow", "blue", "green", "red")) +  # Added color for all regions
  theme_minimal() +
  coord_fixed(ratio = 1)  

```

\(c\)

```{r}
# Define the circle equation
circle_eq <- function(X1) {
  sqrt(4 - (X1 + 1)^2) + 2  # Solve for X2
}

# Create X1 values and calculate corresponding X2 values
X1_values <- seq(-3, 1, by = 0.1)
X2_values_positive <- sapply(X1_values, function(X1) circle_eq(X1))
X2_values_negative <- sapply(X1_values, function(X1) -circle_eq(X1) + 4)

# Create a data frame for the circle
circle_data <- data.frame(X1 = rep(X1_values, 2),
                          X2 = c(X2_values_positive, X2_values_negative),
                          Type = rep(c("Upper half", "Lower half"), each = length(X1_values)))

# Points for classification
specific_points <- data.frame(
  X1 = c(0, -1, 2, 3),  # X1 values
  X2 = c(2, 1, 2, 8)    # X2 values
)

# Classify the points
specific_points$classification <- ifelse(
  (1 + specific_points$X1)^2 + (2 - specific_points$X2)^2 > 4, "(1 + X1)^2 + (2 - X2)^2 > 4", "(1 + X1)^2 + (2 - X2)^2 ≤ 4"
)

# Plot the circle and points with different colors for the regions
ggplot() +
  geom_line(data = circle_data, aes(x = X1, y = X2, color = Type), size = 1
            ) +  # Plot circle
  geom_point(data = specific_points, aes(x = X1, y = X2, color = classification), size = 3) +  # Plot classified points
  labs(title = "Classified Points for the Circle Equation", x = "X1", y = "X2") +
  scale_color_manual(values = c("(1 + X1)^2 + (2 - X2)^2 > 4" = "blue", "(1 + X1)^2 + (2 - X2)^2 ≤ 4" = "red")) +  # Set colors for blue and red
  theme_minimal() +
  coord_fixed(ratio =1)


```

\(d\) The decision boundary in part (c) is given by the equation:$(1+X_1)^2+(2-X_2)^2 = 4$. After expand above equation, we get:

$1+{X_1}^2+2X_1+4+{X_2}^2-4X_2 = 4$

${X_1}^2+{X_2}^2+2X_1-4X_2 = 0$

In this equation, we have these items $X_1,X_2,{X_1}^2,{X_2}^2$.

This shows that the decision boundary is quadratic when expressed in terms of the original features $X_1$ and $X_2$. However, if we introduce new features such as ${X_1}^2$ and ${X_2}^2$ (which are the squared versions of $X_1$ and $X_2$), the decision boundary becomes linear in terms of these new features because the equation will now only involve linear terms in $X_1$, $X_2$, ${X_1}^2$, and ${X_2}^2$.

## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.

------------------------------------------------------------------------

Solution:

```{r}
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ISLR2)
library(caret)
library(doParallel)
library(vip)
library(doParallel)
```

```{r}
# load the data
data("Carseats", package = "ISLR2")
Carseats$Sales<- ifelse(Carseats$Sales > 8, "High", "Low")
Carseats$Sales <- as.factor(Carseats$Sales)
```

#### **Initial split into test and non-test sets**

```{r}
#Initial split into test and non-test sets
set.seed(212)
data_split <- initial_split(
  Carseats, 
  prop = 0.75,
  strata = Sales
  )
data_split

Carseats_other <- training(data_split)
dim(Carseats_other)

Carseats_test <- testing(data_split)
dim(Carseats_test)
```

#### Recipe

```{r}
svm_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) # %>%
  # estimate the means and standard deviations
  # prep(training = Heart_other, retain = TRUE)
svm_recipe
```

### SVM with linear kernel

#### Model & Workflow

```{r}
svm_linear_mod <- 
  svm_linear(
    mode = "classification",
    cost = tune()
) %>%
  set_engine("kernlab")
svm_linear_mod

svm_linear_wf <- workflow() %>% 
  add_recipe(svm_recipe) %>%
  add_model(svm_linear_mod)
```

#### Tuning grid

```{r}
param_grid <- grid_regular(
  cost(range = c(-3, 2)),
  #scale_factor(range = c(-1, 1)),
  levels = c(5)
  )
param_grid
```

#### Cross-validation

```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds
# Fit cross-validation
svm_linear_fit <- 
  svm_linear_wf %>% 
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
```

```{r}
#Visualize CV results
svm_linear_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = cost, y = mean)) + 
  geom_line() +
  geom_point() +
  labs(x = "Cost",
       y = "CV AUC") +
  scale_x_log10()
```

```{r}
best_linear <- svm_linear_fit %>% 
  select_best(metric = "roc_auc")
best_linear
```

#### Finalize the model

```{r}
# Final workflow
final_linear_wf <- 
  svm_linear_wf %>% 
  finalize_workflow(
    best_linear
  )
# fit the whole training set, then predict test 
final_linear_fit <- 
  final_linear_wf %>% 
  last_fit(data_split)

# test metrics
final_linear_fit %>% 
  collect_metrics()
```

#### Visualize the final model

```{r}
set.seed(212)
split_obj <- initial_split(data = Carseats, prop = 0.75, strata = Sales)
train <- training(split_obj)
test <- testing(split_obj)

# Create the recipe
recipe(Sales ~ ., data = train) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep() -> recipe_obj

# Bake
train <- bake(recipe_obj, new_data=train)
test <- bake(recipe_obj, new_data=test)
```

```{r}
final_linear_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  # vip(method = "permute", train= Heart)
  vip(method = "permute", 
      target = "Sales", metric = "accuracy",
      pred_wrapper = kernlab::predict, train = train)
```

```{r}
# Use svm_linear() for linear kernel SVM
svm_linear_spec <- svm_linear() %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Fit the model with linear kernel
svm_linear_fit <- svm_linear_spec %>%
  fit(Sales ~ ., data = train[, c('Price', 'ShelveLoc_Good', 'Sales')])

# Visualize the decision boundary
svm_linear_fit %>%
  extract_fit_engine() %>%
  plot()
```

### SVM with polynomial kernel

#### Model & Workflow

```{r}
svm_mod <- 
  svm_poly(
    mode = "classification",
    cost = tune(),
    degree = tune()
  ) |>
  set_engine("kernlab")
svm_mod

svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf
```

#### Tuning grid

```{r}
param_grid <- grid_regular(
  cost(range = c(-3, 3)),
  degree(range = c(1, 5)),
  levels = c(5)
  )
param_grid
```

#### Cross-validation

```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds

svm_fit <- svm_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit
```

```{r}
#Visualize CV results
svm_fit %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = degree, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()

```

```{r}
svm_fit %>%
  show_best(metric = "roc_auc")

best_svm <- svm_fit %>%
  select_best(metric = "roc_auc")
best_svm

```

#### Finalize the model

```{r}
final_wf <- svm_wf %>%
  finalize_workflow(best_svm)
final_wf

final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

final_fit %>% 
  collect_metrics()

```

#### Visualize the final model

```{r}
set.seed(212)
split_obj <- initial_split(data = Carseats, prop = 0.75, strata = Sales)
train <- training(split_obj)
test <- testing(split_obj)


# Create the recipe
recipe(Sales ~ ., data = train) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep() -> recipe_obj

# Bake
train <- bake(recipe_obj, new_data=train)
test <- bake(recipe_obj, new_data=test)
```

```{r}
final_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(method = "permute", 
      target = "Sales", metric = "accuracy",
      pred_wrapper = kernlab::predict, train = train)
```

```{r}
# Use svm_poly() for polynomial kernel SVM
svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_rbf_fit <- svm_rbf_spec %>%
  fit(Sales ~ ., data = train[, c('Price', 'ShelveLoc_Good', 'Sales')])

svm_rbf_fit %>%
  extract_fit_engine() %>%
  plot()
```

### SVM with radial kernel

#### Model & Workflow

```{r}
svm_mod <- 
  svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod

svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf
```

#### Tuning grid

```{r}
param_grid <- grid_regular(
  cost(range = c(-8, 5)),
  rbf_sigma(range = c(-5, -3)),
  levels = c(14, 5)
  )
param_grid
```

#### Cross-validation

```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds

svm_fit <- svm_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit
```

```{r}
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = cost, y = mean, alpha = rbf_sigma)) +
  geom_point() +
  geom_line(aes(group = rbf_sigma)) +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10() 

```

```{r}
svm_fit %>%
  show_best(metric = "roc_auc")

best_svm <- svm_fit %>%
  select_best(metric = "roc_auc")
best_svm
```

#### Finalize the model

```{r}
# Final workflow
final_wf <- svm_wf %>%
  finalize_workflow(best_svm)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()

# Test metrics
final_fit %>% 
  collect_metrics()
```

#### Visualize the final model

```{r}
set.seed(212)
split_obj <- initial_split(data = Carseats, prop = 0.75, strata = Sales)
train <- training(split_obj)
test <- testing(split_obj)


# Create the recipe
recipe(Sales ~ ., data = train) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep() -> recipe_obj

# Bake
train <- bake(recipe_obj, new_data=train)
test <- bake(recipe_obj, new_data=test)
```

```{r}
final_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(method = "permute", 
      target = "Sales", metric = "accuracy",
      pred_wrapper = kernlab::predict, train = train)
```

```{r}
# Use svm_rbf() for RBF kernel SVM
svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Fit the model with RBF kernel
svm_rbf_fit <- svm_rbf_spec %>%
  fit(Sales ~ ., data = train[, c('Price', 'ShelveLoc_Good', 'Sales')])

# Visualize the decision boundary
svm_rbf_fit %>%
  extract_fit_engine() %>%
  plot()


```

### Conlusion:

After comparing the accuracy and roc_auc of each model, the SVM with radial kernel has the highest value, so we choose it as the final model.

## Bonus (10pts)

Let $$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$ Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$.
