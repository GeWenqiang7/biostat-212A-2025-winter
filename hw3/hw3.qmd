---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 18, 2025 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: today
format:
  # pdf:
  #   documentclass: article
  #   toc: true
  #   toc-depth: 2
  #   keep-tex: true
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)

![](images/clipboard-2289660795.png){width="611"}

![](images/clipboard-3404298366.png){width="548"}

------------------------------------------------------------------------

Solution:

\(a\) The first bootstrap sample is selected randomly from the set of $n$ observations, with replacement. The probability that the first sample is not the $j-th$ observation from the original sample is :

$P\ (first\ sample\ is\ not\ j\ )\ =\ \frac{n-1}{n}$

This is because there are $n−1$ other possible observations that could be selected out of the total $n$.

\(b\) The second bootstrap sample is selected randomly from the set of $n$ observations, with replacement. The probability that the first sample is not the $j-th$ observation from the original sample is :

$P\ (second\ sample\ is\ not\ j\ )\ =\ \frac{n-1}{n}$

It has the same probability of the first sample is not the $j-th$ observation.

\(c\) Since each observation is selected independently with replacement, the probability that the $j-th$ observation is not selected in any specific sample is $(1− \frac{1}{n})$. Therefore, the probability that the $j-th$ observation is not in the bootstrap sample is:

$P\ (\ j-th\ observation\ is\ not\ in\ sample\ )\ =\ (1\ -\frac{1}{n})^n$.

(d)The probability that the $j-th$ observation is included in the bootstrap sample when $n=5$:

$P\ (\ j-th\ observation\ is\ in\ sample\ )\ =\ 1- (\ 1-\frac{1}{5})^5 \ \approx\ 0.672$

(e)The probability that the $j-th$ observation is included in the bootstrap sample when $n=100$:

$P\ (\ j-th\ observation\ is\ in\ sample\ )\ =\ 1- (\ 1-\frac{1}{100})^{100} \ \approx\ 0.634$

(f)The probability that the $j-th$ observation is included in the bootstrap sample when $n=10,000$:

$P\ (\ j-th\ observation\ is\ in\ sample\ )\ =\ 1- (\ 1-\frac{1}{10,000})^{10,000} \ \approx\ 0.632$

\(g\)

```{r}
library(ggplot2)

n_values <- 1:10000
probabilities <- 1 - (1 - 1/n_values)^n_values
data <- data.frame(n = n_values, probability = probabilities)

ggplot(data, aes(x = n, y = probability)) +
  geom_line(color = "blue") + 
  scale_x_log10() +
  labs(x = "Sample size (n)", 
       y = "Probability", 
       title = "Probability that the j-th observation is in the bootstrap sample") +
  theme_minimal()



```

For small sample sizes (n \< 10), the probability is significantly lower. With very few observations, the chance of a specific observation being included in the bootstrap sample is quite low. As n increases, the probability gradually increases towards 1. The rate of this increase slows down as n gets larger, and for sufficiently large n, the probability converges near 1. The rapid increase at the start might be due to the steep nature of the function for small n. Using a logarithmic scale for the x-axis can help visualize the change more effectively for smaller values of n.

\(h\)

```{r}
set.seed(10)
n <- 100
num_simulations <- 10000

store <- numeric(num_simulations)

for (i in 1:num_simulations) {
  sample <- sample(1:n, n, replace = TRUE)  
  store[i] <- sum(sample == 4) > 0  
}

probability <- mean(store)
print(probability)
```

The simulation result gives a probability of approximately 0.6279. This means, based on the 10,000 simulations, the 5th observation is included in about 62.79% of the bootstrap samples.

This probability shows how often the 5th observation appears in the bootstrap samples.Since bootstrap sampling is done with replacement, each observation has a chance of being repeated in each sample, and hence the probability reflects how likely it is for a specific observation (in this case, the 5th) to appear in a given sample.

## ISL Exercise 5.4.9 (20pts)

![](images/clipboard-858485636.png){width="593"}

![](images/clipboard-2128582213.png){width="607"}

------------------------------------------------------------------------

Solution:

\(a\)

```{r}
library(ISLR)
library(MASS)

data("Boston")
mu_hat <- mean(Boston$medv)
print(mu_hat)
```

\(b\)

```{r}
# Calculate standard deviation of medv
s <- sd(Boston$medv)

# Calculate the number of observations
n <- length(Boston$medv)

# Calculate the standard error of the mean
SE_mu_hat <- s / sqrt(n)
print(SE_mu_hat)

```

\(c\)

```{r}

```

\(d\)

\(e\)

\(f\)

\(g\)

\(h\)

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.1 (10pts)

![](images/clipboard-428825808.png){width="601"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.3 (10pts)

![](images/clipboard-2208331259.png){width="607"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.4 (10pts)

![](images/clipboard-875371946.png){width="613"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.5 (10pts)

![](images/clipboard-4278770393.png){width="599"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.11 (30pts)

![](images/clipboard-2470923067.png){width="560"}

You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare *at least* 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |     |
|:------:|:-------:|:---------:|:---:|
|   LS   |         |           |     |
| Ridge  |         |           |     |
| Lasso  |         |           |     |
|  ...   |         |           |     |

------------------------------------------------------------------------

Solution:

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that $$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$

------------------------------------------------------------------------

Solution:
