---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 18, 2025 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: today
format:
  # pdf:
  #   documentclass: article
  #   toc: true
  #   toc-depth: 2
  #   keep-tex: true
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)

![](images/clipboard-2289660795.png){width="611"}

![](images/clipboard-3404298366.png){width="548"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 5.4.9 (20pts)

![](images/clipboard-858485636.png){width="593"}

![](images/clipboard-2128582213.png){width="607"}

------------------------------------------------------------------------

Solution:

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.1 (10pts)

![](images/clipboard-428825808.png){width="601"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.3 (10pts)

![](images/clipboard-2208331259.png){width="607"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.4 (10pts)

![](images/clipboard-875371946.png){width="613"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.5 (10pts)

![](images/clipboard-4278770393.png){width="599"}

------------------------------------------------------------------------

Solution:

## ISL Exercise 6.6.11 (30pts)

![](images/clipboard-2470923067.png){width="560"}

You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare *at least* 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |     |
|:------:|:-------:|:---------:|:---:|
|   LS   |         |           |     |
| Ridge  |         |           |     |
| Lasso  |         |           |     |
|  ...   |         |           |     |

------------------------------------------------------------------------

Solution:

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that $$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$

------------------------------------------------------------------------

Solution:
