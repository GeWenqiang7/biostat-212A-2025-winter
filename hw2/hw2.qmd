---
title: "Biostat 212a Homework 2"
subtitle: "Due Feb 8, 2025 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  # pdf:
  #   documentclass: article
  #   toc: true
  #   toc-depth: 2
  #   keep-tex: true
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 4.8.1 (10pts)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/clipboard-4077728568.png")
```

```{r, echo=FALSE, out.width="40%", fig.align='center'}
knitr::include_graphics(c("images/clipboard-3304908845.png", "images/clipboard-249183538.png"))
```

------------------------------------------------------------------------

Solution:

![](images/clipboard-2967000257.jpeg){width="660"}

## ISL Exercise 4.8.6 (10pts)

![](images/clipboard-2695304714.png)

------------------------------------------------------------------------

Solution:

![](images/clipboard-3585369922.jpeg){width="666"}

## ISL Exercise 4.8.9 (10pts)

![](images/clipboard-2955947930.png)

------------------------------------------------------------------------

Solution:

![](images/clipboard-3451261091.jpeg){width="675"}

## ISL Exercise 4.8.13 (a)-(i) (50pts)

![](images/clipboard-2962329981.png){width="667"}

![](images/clipboard-3390512393.png){width="661"}

------------------------------------------------------------------------

Solution:

```{r}
library(ISLR2)
library(MASS)

data("Weekly")
# Structure of the dataset
str(Weekly)

```

\(a\)

```{r}
# Numerical summary
summary(Weekly)

# Plot the Volume over time
plot(Weekly$Year, Weekly$Volume, main="Trading Volume Over Time", xlab="Year", ylab="Volume", col="blue", pch=20)

# Boxplot of market return (Today) by Direction
boxplot(Today ~ Direction, data=Weekly, main="Market Return by Direction", ylab="Today’s Return", col=c("red", "green"))

# Correlation matrix (excluding categorical variables)
cor(Weekly[, -9])  # Exclude the Direction column

```

Volume has increased significantly over time. Returns (Today) have a mean near zero, indicating relatively balanced ups and downs in market movement. The Direction variable (Up/Down) suggests a roughly even split, meaning the market is not strongly biased in one direction.

Upward (Up) and downward (Down) movements have different distributions. Down days have more extreme negative outliers, indicating higher risk when the market declines. The median return is slightly higher for "Up" movements, but variance is similar for both.

\(b\)

```{r}
# Fit logistic regression model
logistic_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
                      data = Weekly, family = binomial)

# Summary of the logistic regression model
summary(logistic_model)
```

$log(\frac{P(Up)}{1-P(Up)})$ = 0.26686−0.04127 × Lag1+0.05844 × Lag2−0.01066 × Lag3−0.02779 × Lag4−0.01447 × Lag5−0.02274 × Volume.

Yes, the p-values of Lag1 and Lag3 are less than 0.05, so they are statistically significant.

\(c\)

```{r}
# Predict probabilities
pred_probs <- predict(logistic_model, type="response")

# Convert probabilities to class predictions (threshold = 0.5)
pred_classes <- ifelse(pred_probs > 0.5, "Up", "Down")

# Create confusion matrix
conf_matrix <- table(Predicted = pred_classes, Actual = Weekly$Direction)

# Compute accuracy
accuracy <- mean(pred_classes == Weekly$Direction)

# Print results
print(conf_matrix)
print(paste("Overall accuracy:", round(accuracy, 4)))

```

True Positives (TP) = 557 ; False Positives (FP) = 430 ; True Negatives (TN) = 54 ; False Negatives (FN) = 48

Accuracy= $\frac{TP+TN}{Total Samples} = \frac{557+54}{54+48+430+557}=0.5611$ . This is only slightly better than random guessing (50%).

The model is biased towards predicting "Up", as indicated by the large number of false positives (FP = 430). The model fails to predict "Down" accurately, with only 54 correct "Down" predictions out of 484 actual "Down" instances.

\(d\)

```{r}
# Split the dataset
train <- Weekly$Year < 2009
train_data <- Weekly[train, ]
test_data <- Weekly[!train, ]

# Fit logistic regression using Lag2
logistic_model_lag2 <- glm(Direction ~ Lag2, data=train_data, family=binomial)

# Predict on test data
test_probs <- predict(logistic_model_lag2, newdata=test_data, type="response")

# Convert probabilities to class labels
test_preds <- ifelse(test_probs > 0.5, "Up", "Down")

# Compute confusion matrix
conf_matrix_test <- table(Predicted = test_preds, Actual = test_data$Direction)

# Compute accuracy
test_accuracy <- mean(test_preds == test_data$Direction)

# Print results
print(conf_matrix_test)
print(paste("Test accuracy:", round(test_accuracy, 4)))


```

\(e\)

```{r}
# Fit LDA model
lda_model <- lda(Direction ~ Lag2, data=train_data)

# Predict on test data
lda_preds <- predict(lda_model, newdata=test_data)

# Extract class predictions
lda_classes <- lda_preds$class

# Create confusion matrix
conf_matrix_lda <- table(Predicted = lda_classes, Actual = test_data$Direction)

# Compute accuracy
lda_accuracy <- mean(lda_classes == test_data$Direction)

# Print results
print(conf_matrix_lda)
print(paste("LDA test accuracy:", round(lda_accuracy, 4)))

```

\(f\)

```{r}
# Fit QDA model
qda_model <- qda(Direction ~ Lag2, data=train_data)

# Predict on test data
qda_preds <- predict(qda_model, newdata=test_data)

# Extract class predictions
qda_classes <- qda_preds$class

# Compute confusion matrix
conf_matrix_qda <- table(Predicted = qda_classes, Actual = test_data$Direction)

# Compute accuracy
qda_accuracy <- mean(qda_classes == test_data$Direction)

# Print results
print(conf_matrix_qda)
print(paste("QDA test accuracy:", round(qda_accuracy, 4)))

```

\(g\)

```{r}
library(class)

# Prepare training and test data
train_X <- train_data$Lag2
test_X <- test_data$Lag2
train_Y <- train_data$Direction
test_Y <- test_data$Direction

# Apply KNN with K=1
knn_preds <- knn(train = matrix(train_X), test = matrix(test_X), 
                 cl = train_Y, k = 1)

# Compute confusion matrix
conf_matrix_knn <- table(Predicted = knn_preds, Actual = test_Y)

# Compute accuracy
knn_accuracy <- mean(knn_preds == test_Y)

# Print results
print(conf_matrix_knn)
print(paste("KNN (K=1) test accuracy:", round(knn_accuracy, 4)))

```

\(h\)

```{r}
library(e1071)

# Fit Naive Bayes model
nb_model <- naiveBayes(Direction ~ Lag2, data=train_data)

# Predict on test data
nb_preds <- predict(nb_model, newdata=test_data)

# Compute confusion matrix
conf_matrix_nb <- table(Predicted = nb_preds, Actual = test_data$Direction)

# Compute accuracy
nb_accuracy <- mean(nb_preds == test_data$Direction)

# Print results
print(conf_matrix_nb)
print(paste("Naive Bayes test accuracy:", round(nb_accuracy, 4)))

```

\(i\)

```{r}
# Create a comparison table
model_comparison <- data.frame(
  Model = c("Logistic Regression", "LDA", "QDA", "KNN (K=1)", 
            "Naive Bayes"),
  Accuracy = c(test_accuracy, lda_accuracy, qda_accuracy, 
               knn_accuracy, nb_accuracy)
)

# Print comparison results
print(model_comparison)

```

The Logistic Regression and LDA appear to have the best results on this data, and they both have 0.625 accuracy.

## Bonus question: ISL Exercise 4.8.13 Part (j) (30pts)

![](images/clipboard-698312542.png)

------------------------------------------------------------------------

Solution:

\(j\) Logistic Regression with multiple predictors

```{r}
logistic_model_extended <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
                               data=train_data, family=binomial)

# Predictions
test_probs_extended <- predict(logistic_model_extended, newdata=test_data, type="response")
test_preds_extended <- ifelse(test_probs_extended > 0.5, "Up", "Down")

# Confusion Matrix
conf_matrix_logistic_extended <- table(Predicted = test_preds_extended, Actual = test_data$Direction)
logistic_accuracy_extended <- mean(test_preds_extended == test_data$Direction)

print(conf_matrix_logistic_extended)
print(paste("Extended Logistic Regression Accuracy:", round(logistic_accuracy_extended, 4)))

```

Logistic Regression with interaction terms

```{r}
logistic_model_interaction <- glm(Direction ~ Lag2 * Volume, 
                                  data=train_data, family=binomial)

# Predictions
test_probs_interaction <- predict(logistic_model_interaction, 
                                  newdata=test_data, type="response")
test_preds_interaction <- ifelse(test_probs_interaction > 0.5, "Up", "Down")

# Confusion Matrix
conf_matrix_logistic_interaction <- table(Predicted = test_preds_interaction, 
                                          Actual = test_data$Direction)
logistic_accuracy_interaction <- mean(
  test_preds_interaction == test_data$Direction
  )

print(conf_matrix_logistic_interaction)
print(paste("Logistic Regression with Interaction Accuracy:", 
            round(logistic_accuracy_interaction, 4)))


```

LDA with More Predictors

```{r}
library(MASS)
lda_model_extended <- lda(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
                          data=train_data)

# Predictions
lda_preds_extended <- predict(lda_model_extended, newdata=test_data)$class

# Confusion Matrix
conf_matrix_lda_extended <- table(Predicted = lda_preds_extended, Actual = 
                                    test_data$Direction)
lda_accuracy_extended <- mean(lda_preds_extended == test_data$Direction)

print(conf_matrix_lda_extended)
print(paste("Extended LDA Accuracy:", round(lda_accuracy_extended, 4)))

```

QDA with More Predictors

```{r}
qda_model_extended <- qda(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
                          data=train_data)

# Predictions
qda_preds_extended <- predict(qda_model_extended, newdata=test_data)$class

# Confusion Matrix
conf_matrix_qda_extended <- table(Predicted = qda_preds_extended, Actual = 
                                    test_data$Direction)
qda_accuracy_extended <- mean(qda_preds_extended == test_data$Direction)

print(conf_matrix_qda_extended)
print(paste("Extended QDA Accuracy:", round(qda_accuracy_extended, 4)))

```

Tuning K for KNN

```{r}
library(class)

# Function to evaluate KNN for different K values
knn_evaluate <- function(k) {
  knn_preds <- knn(train=as.matrix(train_data[, c("Lag2")]), 
                   test=as.matrix(test_data[, c("Lag2")]), 
                   cl=train_data$Direction, k=k)
  
  conf_matrix_knn <- table(Predicted = knn_preds, Actual = test_data$Direction)
  knn_accuracy <- mean(knn_preds == test_data$Direction)
  
  return(list(conf_matrix=conf_matrix_knn, accuracy=knn_accuracy))
}

# Experiment with different values of K
knn_results <- lapply(c(1, 3, 5, 7, 10, 15, 20), knn_evaluate)

# Print results for each K
for (i in 1:length(knn_results)) {
  print(paste("KNN with K =", c(1, 3, 5, 7, 10, 15, 20)[i]))
  print(knn_results[[i]]$conf_matrix)
  print(paste("Accuracy:", round(knn_results[[i]]$accuracy, 4)))
}

```

Naive Bayes with More Predictors

```{r}
library(e1071)
nb_model_extended <- naiveBayes(Direction ~ Lag1 + Lag2 + 
                                  Lag3 + Lag4 + Lag5 + Volume, data=train_data)

# Predictions
nb_preds_extended <- predict(nb_model_extended, newdata=test_data)

# Confusion Matrix
conf_matrix_nb_extended <- table(Predicted = nb_preds_extended, Actual = 
                                   test_data$Direction)
nb_accuracy_extended <- mean(nb_preds_extended == test_data$Direction)

print(conf_matrix_nb_extended)
print(paste("Extended Naive Bayes Accuracy:", round(nb_accuracy_extended, 4)))

```

Comparing All Models

```{r}
# Create a comparison table
model_comparison <- data.frame(
  Model = c("Logistic Regression", "logistic_model_extended", 
            "Logistic Regression (Interaction)", 
            "LDA", "LDA (Extended)", "QDA", "QDA (Extended)", 
            "KNN (K=1)","KNN (K=3)", "KNN (K=5)", "KNN (K=7)", 
            "KNN (K=10)", "KNN (K=15)", "KNN (K=20)", 
            "Naive Bayes", "Naive Bayes (Extended)"),
  Accuracy = c(test_accuracy, logistic_accuracy_extended, 
               logistic_accuracy_interaction, 
               lda_accuracy, lda_accuracy_extended, 
               qda_accuracy, qda_accuracy_extended, 
               knn_results[[1]]$accuracy, knn_results[[2]]$accuracy, 
               knn_results[[3]]$accuracy, knn_results[[4]]$accuracy, 
               knn_results[[5]]$accuracy, knn_results[[6]]$accuracy, 
               knn_results[[7]]$accuracy,
               nb_accuracy, nb_accuracy_extended)
)

# Print comparison results
print(model_comparison)
```

## Bonus question: ISL Exercise 4.8.4 (30pts)

```{r, echo=FALSE, out.width="71%", fig.align='center'}
knitr::include_graphics(("images/clipboard-4051626028.png"))
```

```{r, echo=FALSE, out.width="60%", fig.align='center'}
knitr::include_graphics(("images/clipboard-3443800866.png"))
```

------------------------------------------------------------------------

Solution:

\(a\) Since the feature $X$ is uniformly distributed in the range$[0,1]$, We consider a test observation and use only those within 10% of its range.

If a test point is at $𝑋 = 0.6$, we use observations in the range: $[ 0.55 , 0.65]$. The total range is 1, so the fraction of data used is$\frac{selected\ range}{total\ range} = \frac{0.65 − 0.55}{1} = 0.1$. Therefore, we use 10% of the data.

\(b\) We now have two features, $(X_1,X_2)$, both uniformly distributed on $[ 0 , 1] × [ 0 , 1]$.

To predict the response, we use only observations within 10% of both $X_1$ and $X_2$.

When $X_1=0.6,\ X_2=0.35$, we will use: $X_1\in[0.55,0.65] ,\ X_2 \in [0.3,0.4]$.

The fraction of data used is $\frac{selected\ range}{total\ range} = \frac{0.10 * 0.10 }{1} = 0.01$. Therefore, we use 1% of the data.

\(c\)

Now, we have 100 features, all uniformly distributed in $[0,1]$. We use only the observations within 10% of each feature's range.

For each feature $X_i$, we select observations within: $[X_i−0.05,\ X_i+0.05]$ (assuming the test observation is not too close to the boundaries).

Since the features are independent, the fraction of observations in each dimension is 0.1. The fraction of data used is $(0.1)^{100}$, which is very small number.

Therefore, in 100D space, almost none of the training data is "close" to the test point, making KNN ineffective.

\(d\)

From previous results, in 1D, we use 10% of the data; In 2D, we use 1%; In 100D, we use $(0.1)^{100}$, which is practically zero.

Because most points are far away, and distances between them are almost uniform, and even with millions of points, find "near" neighbors is unlikely. What's more, searching for neighbors in high-dimensional space is inefficient.

Therefore, KNN performs poorly in high dimensions because there are too few nearby training points, making predictions unreliable.

\(e\)

We define a p-dimensional hypercube centered at a test point that contains 10% of the total training data.

Volume of the hypercube in $p-dimensional$ space is: $Fraction\ of\ Data\ Used = ( Side\ Length )^𝑝$

Let $s$ be the side length of the hypercube. To contain 10% of data, we solve: $s^p = 0.1$.

For $𝑝= 1$: $s^1 = 0.1$, $s=0.1$. In 1D, we take 10% of the range.

For $𝑝= 2$: $s^2 = 0.1$, $s=\sqrt{0.1} \approx 0.316$. In 2D, the square has side length around 0.316, much larger than in 1D.

For $𝑝= 100$: $s^{100} = 0.1$, $s=0.1^{\frac{1}{100}} = 10^{-0.01} \approx 0.977$. In 100D, the side length is about 0.977, meaning almost the entire space is included.

In high dimensions, the hypercube must be almost the entire space to contain just 10% of the data. This confirms why KNN is ineffective in high dimensions: Everything is "far" apart, so the notion of "nearest neighbors" breaks down.

Thus, KNN works well in low dimensions because neighbors are meaningful. In high dimensions, the space becomes too sparse, making KNN ineffective.
