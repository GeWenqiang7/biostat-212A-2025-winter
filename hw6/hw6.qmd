---
title: "Biostat 212a Homework 6"
subtitle: "Due Mar 21, 2025 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

Load R libraries.

```{r}
library(tidyverse)
library(tidymodels)
library(readr)
library(tswge)
library(ggplot2)

acfdf <- function(vec) {
    vacf <- acf(vec, plot = F)
    with(vacf, data.frame(lag, acf))
}

ggacf <- function(vec) {
    ac <- acfdf(vec)
    ggplot(data = ac, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + 
        geom_segment(mapping = aes(xend = lag, yend = 0))
}

tplot <- function(vec) {
    df <- data.frame(X = vec, t = seq_along(vec))
    ggplot(data = df, aes(x = t, y = X)) + geom_line()
}
```

## New York Stock Exchange (NYSE) data (1962-1986) (140 pts)

::: {#fig-nyse}
![](images/clipboard-4137452334.png)

Historical trading statistics from the New York Stock Exchange. Daily values of the normalized log trading volume, DJIA return, and log volatility are shown for a 24-year period from 1962-1986. We wish to predict trading volume on any day, given the history on all earlier days. To the left of the red bar (January 2, 1980) is training data, and to the right test data.
:::

The [`NYSE.csv`](https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/NYSE.csv) file contains three daily time series from the New York Stock Exchange (NYSE) for the period Dec 3, 1962-Dec 31, 1986 (6,051 trading days).

-   `Log trading volume` ($v_t$): This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.

-   `Dow Jones return` ($r_t$): This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.

-   `Log volatility` ($z_t$): This is based on the absolute values of daily price movements.

```{r}
# Read in NYSE data from url

url = "https://raw.githubusercontent.com/ucla-biostat-212a/2025winter/master/slides/data/NYSE.csv"
NYSE <- read_csv(url)

NYSE
```

The **autocorrelation** at lag $\ell$ is the correlation of all pairs $(v_t, v_{t-\ell})$ that are $\ell$ trading days apart. These sizable correlations give us confidence that past values will be helpful in predicting the future.

```{r}
#| code-fold: true
#| label: fig-nyse-autocor
#| fig-cap: "The autocorrelation function for log volume. We see that nearby values are fairly strongly correlated, with correlations above 0.2 as far as 20 days apart."

ggacf(NYSE$log_volume) + ggthemes::theme_few()

```

Do a similar plot for (1) the correlation between $v_t$ and lag $\ell$ `Dow Jones return` $r_{t-\ell}$ and (2) correlation between $v_t$ and lag $\ell$ `Log volatility` $z_{t-\ell}$.

```{r}
seq(1, 30) %>% 
  map(function(x) {cor(NYSE$log_volume , lag(NYSE$DJ_return, x), use = "pairwise.complete.obs")}) %>% 
  unlist() %>% 
  tibble(lag = 1:30, cor = .) %>% 
  ggplot(aes(x = lag, y = cor)) + 
  geom_hline(aes(yintercept = 0)) + 
  geom_segment(mapping = aes(xend = lag, yend = 0)) + 
  ggtitle("AutoCorrelation between `log volume` and lagged `DJ return`")
```

```{r}
seq(1, 30) %>% 
  map(function(x) {cor(NYSE$log_volume , lag(NYSE$log_volatility, x), use = "pairwise.complete.obs")}) %>% 
  unlist() %>% 
  tibble(lag = 1:30, cor = .) %>% 
  ggplot(aes(x = lag, y = cor)) + 
  geom_hline(aes(yintercept = 0)) + 
  geom_segment(mapping = aes(xend = lag, yend = 0)) + 
  ggtitle("AutoCorrelation between `log volume` and lagged `log volatility`")
```

### Project goal

Our goal is to forecast daily `Log trading volume`, using various machine learning algorithms we learnt in this class.

The data set is already split into train (before Jan 1st, 1980, $n_{\text{train}} = 4,281$) and test (after Jan 1st, 1980, $n_{\text{test}} = 1,770$) sets.

<!-- Include `day_of_week` as a predictor in the models. -->

In general, we will tune the lag $L$ to acheive best forecasting performance. In this project, we would fix $L=5$. That is we always use the previous five trading days' data to forecast today's `log trading volume`.

Pay attention to the nuance of splitting time series data for cross validation. Study and use the [`time-series`](https://www.tidymodels.org/learn/models/time-series/) functionality in tidymodels. Make sure to use the same splits when tuning different machine learning algorithms.

Use the $R^2$ between forecast and actual values as the cross validation and test evaluation criterion.

### Baseline method (20 pts)

We use the straw man (use yesterdayâ€™s value of `log trading volume` to predict that of today) as the baseline method. Evaluate the $R^2$ of this method on the test data.

------------------------------------------------------------------------

Solution:

```{r}
# Lag: look back L trading days
L = 5

for(i in seq(1, L)) {
  NYSE <- NYSE %>% 
    mutate(!!paste("DJ_return_lag", i, sep = "") := lag(NYSE$DJ_return, i),
           !!paste("log_volume_lag", i, sep = "") := lag(NYSE$log_volume, i),
           !!paste("log_volatility_lag", i, sep = "") := lag(NYSE$log_volatility, i))
}

NYSE <-   NYSE |>
  na.omit()

print(NYSE , width = Inf)
```

```{r}
# Split the training and tesing set
NYSE_train <- NYSE |>
  filter(train == 'TRUE') |>
  select(-train) |>
  drop_na()

NYSE_test <- NYSE |>
  filter(train == 'FALSE') |>
  select(-train)|>
  drop_na()

dim(NYSE_train)
dim(NYSE_test)
```

```{r}
library(yardstick)

r2_test_strawman <- rsq_vec(NYSE_test$log_volume, 
                            lag(NYSE_test$log_volume, 
                                1)) |>
  round(2)

print(paste("Straw man test R2: ", r2_test_strawman))

```

### Autoregression (AR) forecaster (30 pts)

-   Let $$
    y = \begin{pmatrix} v_{L+1} \\ v_{L+2} \\ v_{L+3} \\ \vdots \\ v_T \end{pmatrix}, \quad M = \begin{pmatrix}
    1 & v_L & v_{L-1} & \cdots & v_1 \\
    1 & v_{L+1} & v_{L} & \cdots & v_2 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & v_{T-1} & v_{T-2} & \cdots & v_{T-L}
    \end{pmatrix}.
    $$

-   Fit an ordinary least squares (OLS) regression of $y$ on $M$, giving $$
    \hat v_t = \hat \beta_0 + \hat \beta_1 v_{t-1} + \hat \beta_2 v_{t-2} + \cdots + \hat \beta_L v_{t-L},
    $$ known as an **order-**$L$ autoregression model or **AR(**$L$).

-   Tune AR(5) with elastic net (lasso + ridge) regularization using all 3 features on the training data, and evaluate the test performance.

-   Hint: [Workflow: Lasso](https://ucla-biostat-212a.github.io/2025winter/slides/06-modelselection/workflow_lasso.html) is a good starting point.

------------------------------------------------------------------------

Solution:

```{r}
set.seed(212)
# Load required libraries
library(glmnet)
library(dplyr)
library(tidymodels)
en_recipe <- 
  recipe(log_volume ~ ., data = NYSE_train) |>
  step_dummy(all_nominal(), -all_outcomes()) |> 
  step_normalize(all_numeric_predictors(), -all_outcomes()) |>  
  step_zv(all_predictors()) |>
  step_naomit(all_predictors())  |>
  step_rm(date) |>
  step_indicate_na()
```

```{r}
en_mod <- 
  linear_reg(penalty = tune(), 
             mixture = tune()) |>
  set_engine("glmnet")

en_mod

```

```{r}
en_wf <- 
  workflow() |>
  add_model(en_mod) |>
  add_recipe(en_recipe)
```

```{r}
month_folds <- NYSE_train |>
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4)
```

```{r}
en_grid <-
  grid_regular(penalty(range = c(-8, -7), 
                       trans = log10_trans()), 
               mixture(), 
               levels = c(4,5))
en_grid

en_fit <- tune_grid(en_wf, 
                    resamples = month_folds, 
                    grid = en_grid)

```

```{r}
en_fit |>
  collect_metrics() |>
  filter(.metric == "rsq") |>
  ggplot(mapping = aes(x = penalty, y = mean, 
                       colour = as.factor(mixture))) + 
  geom_point() + 
  labs(x = "Penalty", y = "CV rsq") + 
  scale_x_log10(labels = scales::label_number())

en_fit |>
  show_best()

best_en <- en_fit |>
  select_best(metric = "rsq")
```

```{r}
cv_results <- en_fit |>
  collect_metrics() |>
  filter(.metric == "rsq") |>
  arrange(desc(mean))

print(paste("Straw man CV R2:", round(cv_results$mean[1], 2)))
```

```{r}
#Final model
final_wf <- en_wf |>
  finalize_workflow(best_en)

final_fit <- final_wf |>
  fit(data = NYSE_train)

```

```{r}
predictions <- predict(final_fit, new_data = NYSE_test) |>
  bind_cols(NYSE_test)

metrics <- predictions %>%
  metrics(truth = log_volume, estimate = .pred)
metrics
```

```{r}
rsq_value <- metrics %>%
  filter(.metric == "rsq") %>%
  pull(.estimate)

print(paste("Straw man test R2: ", round(rsq_value,2)))
```

### Random forest forecaster (30pts)

-   Use the same features as in AR($L$) for the random forest. Tune the random forest and evaluate the test performance.

-   Hint: [Workflow: Random Forest for Prediction](https://ucla-biostat-212a.github.io/2025winter/slides/08-tree/workflow_rf_reg.html) is a good starting point.

------------------------------------------------------------------------

Solution:

```{r}
rf_recipe <-
  recipe(log_volume ~ ., data = NYSE_train) |> 
  step_zv(all_predictors()) |>
  step_naomit(all_predictors())  |>
  step_rm(date) |>
  step_indicate_na()
rf_recipe
```

```{r}
rf_mod <- 
  rand_forest(
    mode = "regression",
    mtry = tune(),
    trees = tune()
  ) |>
  set_engine("ranger")
rf_mod

rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
rf_wf
```

```{r}
set.seed(212)
rf_grid <- grid_regular(
  trees(range = c(100L, 300L)),
  mtry(range = c(3L, 5L)),       
  levels = 3
)
```

```{r}
month_folds <- NYSE_train |>
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4)
```

```{r}
rf_fit <- rf_wf |>
  tune_grid(
    resamples = month_folds, 
    grid = rf_grid, 
    metrics = metric_set(rmse, rsq)
  )
```

```{r}
set.seed(212)
rf_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "rsq") |>
  mutate(mtry = as.factor(mtry)) |>
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV rsq")

```

```{r}
rf_fit |>
  show_best()

best_rf <- rf_fit |>
  select_best(metric = "rsq")
best_rf
```

```{r}
cv_results <- rf_fit |>
  collect_metrics() |>
  filter(.metric == "rsq") |>
  arrange(desc(mean))

print(paste("Straw man CV R2:", round(cv_results$mean[1], 2)))
```

```{r}
#Final model
final_wf <- rf_wf |>
  finalize_workflow(best_rf)

final_fit <- final_wf |>
  fit(data = NYSE_train)

```

```{r}
predictions <- predict(final_fit, new_data = NYSE_test) |>
  bind_cols(NYSE_test)

metrics <- predictions |>
  metrics(truth = log_volume, estimate = .pred)
```

```{r}
rsq_value <- metrics %>%
  filter(.metric == "rsq") %>%
  pull(.estimate)
print(paste("Straw man test R2: ", round(rsq_value,2)))
```

### Boosting forecaster (30pts)

-   Use the same features as in AR($L$) for the boosting. Tune the boosting algorithm and evaluate the test performance.

-   Hint: [Workflow: Boosting tree for Prediction](https://ucla-biostat-212a.github.io/2025winter/slides/08-tree/workflow_boosting_reg.html) is a good starting point.

------------------------------------------------------------------------

Solution:

```{r}
gb_recipe <-
  recipe(log_volume ~ ., data = NYSE_train) |> 
  step_dummy(all_nominal(), -all_outcomes()) |>
  step_zv(all_predictors()) |>
  step_naomit(all_predictors())  |>
  step_rm(date) |>
  step_indicate_na()
gb_recipe
```

```{r}
gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 100, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")

gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf
```

```{r}
set.seed(212)
bt_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-2, -0.5), 
             trans = log10_trans()),
  levels = 3
  )

```

```{r}
month_folds <- NYSE_train|>
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4)
```

```{r}
set.seed(212)
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = month_folds,
    grid = bt_grid,
    metrics = metric_set(rmse, rsq)
    )

gb_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "rsq") |>
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV rsq") +
  scale_x_log10()

```

```{r}
gb_fit |>
  show_best()

best_gb <- gb_fit |>
  select_best(metric = "rsq")
best_gb
```

```{r}
cv_results <- gb_fit |>
  collect_metrics() |>
  filter(.metric == "rsq") |>
  arrange(desc(mean))

print(paste("Straw man CV R2:", round(cv_results$mean[1], 2)))
```

```{r}
#Final model
final_wf <- gb_wf |>
  finalize_workflow(best_gb)

```

```{r}
final_fit <- final_wf |>
  fit(data = NYSE_train)

predictions <- predict(final_fit, 
                       new_data = NYSE_test) |>
  bind_cols(NYSE_test)

metrics <- predictions |>
  metrics(truth = log_volume, estimate = .pred)
metrics

```

```{r}

rsq_value <- metrics %>%
  filter(.metric == "rsq") %>%
  pull(.estimate)
print(paste("Straw man test R2: ", round(rsq_value,2)))
```

### Summary (30pts)

Your score for this question is largely determined by your final test performance.

Summarize the performance of different machine learning forecasters in the following format.

|    Method     | CV $R^2$ | Test $R^2$ |     |
|:-------------:|:--------:|:----------:|:---:|
|   Baseline    |    NA    |    0.35    |     |
|     AR(5)     |   0.38   |    0.55    |     |
| Random Forest |   0.35   |    0.51    |     |
|   Boosting    |   0.38   |    0.54    |     |

------------------------------------------------------------------------

Solution:

In this time series forecasting task, different models demonstrated varying levels of generalization ability $(CV\ R^2)$ and predictive performance $(Test\ R^2)$.

**Baseline** relied solely on the previous day's trading volume, lacking generalization capability, and achieved a test $R^2$ of $0.35$, serving as a benchmark.

**AR(5)** utilized data from the past five days for predictions, showing moderate generalization ability $(CV\ R^2 = 0.38)$. Its $Test\ R^2=0.55$ suggests that it effectively captured short-term trends and improved upon the baseline.

**Random Forest (RF)** exhibited slightly lower generalization ability than AR(5) $(CV\ R^2=0.35)$ and achieved a $Test\ R^2=0.51$. This indicates that while RF was able to leverage time-series patterns, it did not significantly surpass the linear AR(5) model.

**Boosting** demonstrated the best generalization ability $(CV\ R^2=0.38)$ and strong predictive performance $Test\ R^2=0.54$, suggesting that it effectively learned the dynamic patterns in trading volume, leading to improved forecasts.

Overall, AR(5) and Boosting provided the highest generalization, with AR(5) showing better predictive accuracy across different datasets.

## ISL Exercise 12.6.13 (90 pts)

![](images/clipboard-2422393102.png)

\(a\)

```{r}
library(ISLR2)
library(tidyverse)
gene <- 
  read.csv("https://www.statlearning.com/s/Ch12Ex13.csv", 
           header=FALSE)
colnames(gene) <- c(paste0("healthy", 1:20),
                    paste0("diseased", 1:20))
head(gene, 10)
```

### 12.6.13 (b) (30 pts)

```{r}
# Compute correlation-based distance matrix
dist_matrix <- as.dist(1 - cor(gene))

# Perform hierarchical clustering with different linkage methods and plot separately
methods <- c("complete", "average", "single")

for (method in methods) {
  hc <- hclust(dist_matrix, method = method)

  # Adjust margins to ensure enough space for labels
  par(mar = c(8, 4, 4, 2))  # Increase bottom margin

  # Plot the dendrogram
  plot(hc, main = paste("Hierarchical Clustering with", method, "Linkage"),
       sub = "", xlab = "", cex = 1, las = 2)  # Reduce font size & rotate labels
}

```

The hierarchical clustering results indicate that the ability to separate healthy and diseased samples depends on the linkage method used. The genes separate the samples into the two groups.

Complete linkage provides the best separation, with most samples correctly grouped, though some misclassification remains.

Average linkage shows moderate separation, but with more mixing between the two groups.

Single linkage performs the worst, suffering from chaining effects that fail to create distinct clusters.

These results suggest that while gene expression patterns contribute to distinguishing the two groups, the effectiveness of clustering is strongly influenced by the chosen linkage method.

### PCA and UMAP (30 pts)

PCA

```{r}
library(tidymodels)
transposed_gene <- as_tibble(t(gene)) |>
  mutate(group = rep(c("healthy", "diseased"), each = 20))
pca_rec <- recipe(~., data = transposed_gene) |>
  update_role(group, new_role = "id") |>
  step_normalize(all_predictors()) |>
  step_pca(all_predictors())
pca_prep <- prep(pca_rec)

library(tidytext)
tidied_pca <- tidy(pca_prep, 2)

tidied_pca |>
  filter(component %in% paste0("PC", 1:4)) |>
  group_by(component) |>
  top_n(8, abs(value)) |>
  ungroup() |>
  mutate(terms = reorder_within(terms, abs(value), component)) |>
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive"
  )

juice(pca_prep) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = group), alpha = 0.7, size = 2) +
  #geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL)

```

UMAP

```{r}
library(embed)
umap_rec <- recipe(~., data = transposed_gene) |>
  update_role(group, new_role = "id") |>
  step_normalize(all_predictors()) |>
  step_umap(all_predictors())
umap_prep <- prep(umap_rec)
umap_prep

juice(umap_prep) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = group), alpha = 0.7, size = 2) +
#  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL)
```

### 12.6.13 (c) (30 pts)

```{r}
library(ggrepel)

# Read the dataset
Ch12Ex13 <- read_csv("https://www.statlearning.com/s/Ch12Ex13.csv", col_names = paste("ID", 1:40, sep = ""))

# Define group variables (1 = diseased, 0 = healthy)
grp <- factor(rep(c(1, 0), each = 20))

# **Optimized computation of Wilcoxon p-values**
wilcox_pvalues <- apply(as.matrix(Ch12Ex13), 1, function(y) {
  wilcox.test(y ~ grp)$p.value  # Perform Wilcoxon rank-sum test for each gene
})

# **Optimized computation of Log2 Fold Change (LFC)**
logFC_values <- rowMeans(as.matrix(Ch12Ex13)[, 1:20]) - rowMeans(as.matrix(Ch12Ex13)[, 21:40])  # Compute mean differences
logFC_values <- log2(exp(logFC_values))  # Convert to Log2 Fold Change

# **Create results table**
out <- tibble(
  gene = rownames(Ch12Ex13),  # Assign gene names
  p_value = wilcox_pvalues,  # Store p-values
  logFC = logFC_values  # Store Log2 Fold Change values
) %>%
  mutate(adj_p_value = p.adjust(p_value, method = "BH"))  

# **Filter significant genes (adj_p_value < 0.05)**
sig_genes <- out %>% filter(adj_p_value < 0.05)

# **Generate the volcano plot**
ggplot(out, aes(x = logFC, y = -log10(p_value))) +
  geom_point(aes(color = adj_p_value < 0.05), alpha = 0.7) + 
  geom_text_repel(data = sig_genes, aes(label = gene), size = 4) + 
  scale_color_manual(values = c("gray", "red")) +  # Define color mapping
  labs(title = "Volcano Plot: Gene Expression Differences",
       x = "Log2 Fold Change (LFC)", y = "-log10(p-value)") +
  theme_minimal()  # Apply a minimalistic theme


```

**Methodology**

I applied the Wilcoxon rank-sum test for statistical analysis and calculated the log2 Fold Change (LFC) to measure the direction and magnitude of gene expression changes. The Wilcoxon test is suitable for non-normally distributed data and is used to assess whether gene expression significantly differs between the two groups. I adjusted the p-values using the Benjamini-Hochberg (BH) method to control the false discovery rate (FDR) and selected genes with adj_p_value \< 0.05 as significantly differentially expressed genes (DEGs). Finally, I visualized the results using a volcano plot, where the x-axis represents the LFC, and the y-axis represents the -log10(p-value), providing an intuitive way to identify genes with significant expression differences between the two groups.

**Findings and Conclusions**

The results from the volcano plot analysis indicate that some genes show significant upregulation or downregulation between the healthy and diseased groups. The red genes indicate significant differences in expression between the two groups and may be potential disease biomarkers. For example, genes 502, 600, 568, and 529 are expressed higher in the healthy group, while genes 135 and 172 are expressed higher in the disease group.

Genes on the left side (LFC \< 0) are more highly expressed in the healthy group, potentially playing a protective role, whereas genes on the right side (LFC \> 0) are more highly expressed in the diseased group, possibly contributing to disease development.
