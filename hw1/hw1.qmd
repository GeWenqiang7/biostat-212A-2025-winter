---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 28, 2025 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## Filling gaps in lecture notes (10% pts)

Consider the regression model $$
Y = f(X) + \epsilon,
$$ where $\operatorname{E}(\epsilon) = 0$.

### Optimal regression function

Show that the choice $$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$ minimizes the mean squared prediction error $$
\operatorname{E}\{[Y - f(X)]^2\},
$$ where the expectations averages over variations in both $X$ and $Y$. (Hint: condition on $X$.)

### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$ can be decomposed as $$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$ where the expectation averages over the variability in $y_0$ and $\hat f$.

## ISL Exercise 2.4.3 (10% pts)

```{r, eval = F}
library(tidyverse)
fit <- lm(sales ~ TV, data = )
```

![](images/clipboard-2321342665.png){width="527"}

\(a\)

```{r}
library(ggplot2)

Flexibility <- 1:100
Bias <- 100 / Flexibility 
Variance <- Flexibility / 10 
TrainingError <- 100 / Flexibility
TestError <- Bias + Variance 
BayesError <- rep(10, 100)  

# Combine into a data frame
data <- data.frame(
  Flexibility = Flexibility,
  Bias = Bias,
  Variance = Variance,
  TrainingError = TrainingError,
  TestError = TestError,
  BayesError = BayesError
)
data_long <- reshape2::melt(data, id.vars = "Flexibility", variable.name = "ErrorType", value.name = "ErrorValue")

ggplot(data_long, aes(x = Flexibility, y = ErrorValue, color = ErrorType)) +
  geom_line(size = 1.2) +
  labs(
    title = "Bias-Variance Decomposition",
    x = "Model Flexibility",
    y = "Error Value",
    color = "Error Components"
  ) +
  scale_color_manual(
    values = c("blue", "red", "green", "purple", "orange"),
    labels = c("Bias (Squared)", "Variance", "Training Error", "Test Error", "Bayes Error")
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  )

```

\(b\) Bias Curve: Decreases because as flexibility increases, the model can better fit the training data, reducing systematic error.Variance Curve: Increases because more flexible models are more sensitive to small fluctuations in the data, leading to overfitting.Training Error Curve: Always decreases because more flexible models can perfectly fit (or nearly fit) the training data.Test Error Curve: U-shaped because it is influenced by both bias and variance. Initially, test error decreases as bias dominates. Later, it increases as variance dominates.Bayes Error: Stays constant as it represents noise or irreducible error in the data.

## ISL Exercise 2.4.4 (10% pts)

![](images/clipboard-1845645679.png)

\(a\) Classification application:

Spam email detection: Response: Email is spam (1) or not (0). Predictors: Frequency of certain keywords, length of the email, etc. Goal: Prediction. The model is used to predict whether new emails are spam or not.

Medical Diagnosis: Response: Whether a patient has a disease Yes (1) or not (0).

Predictors: Age, gender, symptoms, test results, and medical history. Goal: Inference and prediction. Inference is used to understand which predictors are most associated with the disease. Prediction is used to diagnose new patients.

Disease classification: Response variable: the disease classification of the patient, such as diabetes (1) , heart disease (2) , health (0) . Predictive variables: age, blood glucose level, cholesterol level, medical history, lifestyle, etc. Goal: Predict. Assist doctors in making quick diagnostic decisions.

\(b\) Regression application:

Drug dosage and efficacy: Response: Drug efficacy (such as decreased blood glucose levels). Predictive : drug dosage, patient age, weight, etc. Goal: Inference. Assist in drug research, analyze the relationship between dosage and efficacy.

Real estate rent forecast:Response : Rent price. Predictive : geographical location, area, decoration level, surrounding facilities, etc. Goal: Predict. Provide reference prices for the rental market.

Stock market analysis:Response: Future stock price or return. Predictive : historical prices, trading volume, economic indicators, and news sentiment. Goal: Predict. This model helps predict stock prices for investment decisions.

\(c\) Cluster application:

Grouping students based on their academic performance and learning behavior. Features: classroom performance, exam scores, participation, completion of assignments, etc. Goal: To assist teachers in developing teaching plans for different groups. Genotyping analysis

Grouping genes based on DNA sequence data. Features: gene expression level, sequence similarity, etc. Goal: To discover different types of genomic populations for disease research. Retail store location selection

Grouping urban areas based on population density and consumption behavior. Features: Population characteristics (age, income), traffic flow, consumption level, etc. Goal: Help retailers choose the best location.

## ISL Exercise 2.4.10 (30% pts)

Your can read in the `boston` data set directly from url <https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv>. A documentation of the `boston` data set is [here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).

::: panel-tabset
#### R

```{r, evalue = F}
library(tidyverse)
Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

#### Python

```{python, evalue = F}
import pandas as pd
import io
import requests

url = "https://raw.githubusercontent.com/ucla-econ-425t/2023winter/master/slides/data/Boston.csv"
s = requests.get(url).content
Boston = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col = 0)
Boston
```
:::

## ![](images/clipboard-2802351124.png)

\(a\)

```{r}
library(tidyverse)
Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

Done!\
(b)

```{r}
nrow(Boston)  
ncol(Boston)  
```

There are 506 rows and 13 columns. Rows represent suburbs or towns in the Boston area. Each column represents a feature or response variable (e.g., crime rate, tax rate, median value of homes).

\(c\)

```{r}
library(ggplot2)
library(tidyr)
Boston_long <- Boston %>% 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  theme_minimal() +
  labs(title = "Pairwise Scatterplots",
       x = "Lower Status of the Population",
       y = "Median Value of Homes")

pairs(Boston, main = "Pairwise Scatterplots of Boston Dataset")

```

\(d\)

```{r}
cor(Boston$crim, Boston[-1], use = "complete.obs")
```

```{r}
library(corrplot)
corrplot(cor_matrix, method = "color", type = "upper", 
         col = colorRampPalette(c("blue", "white", "red"))(200),
         addCoef.col = "black", number.cex = 0.7, tl.cex = 0.8)


```

Rad is strongly positively correlated with crime rate, indicating that areas with better highway access may attract higher crime rates. Higher property tax rates are also positively correlated with crime. Nitric oxide concentration shows a moderate positive correlation

.Dis (-0.38): Distance from Boston’s employment centers is negatively correlated with crime rates, implying that areas farther from the city center tend to have lower crime rates. Rm: is negatively correlated with crime, suggesting that larger homes are associated with lower crime rates.Zn: Proportion of large residential zones is also negatively correlated.

\(e\)

```{r}
high_crime <- Boston %>% filter(crim > quantile(crim, 0.7))
high_tax <- Boston %>% filter(tax > quantile(tax, 0.7))
high_ptratio <- Boston %>% filter(ptratio > quantile(ptratio, 0.7))
cat("High Crime:", nrow(high_crime), 
    "\nHigh Tax:", nrow(high_tax), 
    "\nHigh Pupil-Teacher Ratio:", nrow(high_ptratio), "\n")
```

\(f\)

```{r}
sum(Boston$chas == 1)  
```

\(g\)

```{r}
median(Boston$ptratio, na.rm = TRUE)

```

\(h\)

```{r}
lowest_medv <- Boston %>% filter(medv == min(medv, na.rm = TRUE))

```

\(i\)

```{r}
seven_rooms <- Boston %>% filter(rm > 7)
eight_rooms <- Boston %>% filter(rm > 8)
nrow(seven_rooms)
nrow(eight_rooms)

```

## ISL Exercise 3.7.3 (20% pts)

![](images/clipboard-2573688108.png)

## ![](images/clipboard-1304642707.png)

(a)-(b)

![](images/17356e791ba29f6f2b80e5588c3b407.jpg)

\(c\) True. The coefficient for the GPA/IQ interaction term is very small, suggesting that the interaction effect is minimal. The contribution of $\hatβ X​4$ =0.01⋅640=6.4, which is relatively small compared to other terms.

## 3.7.15 (20% pts)

![](images/clipboard-3010492794.png)

## ![](images/clipboard-3808075512.png)

\(a\)

\(b\)

\(c\)

## Bonus question (20% pts)

For multiple linear regression, show that $R^2$ is equal to the correlation between the response vector $\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values $\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is $$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$
