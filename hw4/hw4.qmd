---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 4, 2025 @ 11:59PM"
author: "Wenqiang Ge UID:106371961"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
```

## ISL Exercise 8.4.3 (10pts)

![](images/clipboard-2954413279.png)

------------------------------------------------------------------------

Solution:

```{r}
# Define probability range for class 1
p_m1 <- seq(0, 1, length.out = 100)  # Probability values from 0 to 1
p_m2 <- 1 - p_m1  # Probability of the second class

# Compute the three metrics
gini_index <- 1 - (p_m1^2 + p_m2^2)  # Gini index formula
classification_error <- pmin(p_m1, p_m2)  # Classification error (minimum probability)
entropy <- - (p_m1 * log2(p_m1) + p_m2 * log2(p_m2))  # Entropy formula
entropy[is.na(entropy)] <- 0  # Handle log(0) cases (replace NaN with 0)

# Create a dataframe with all values
df <- data.frame(p_m1, gini_index, classification_error, entropy) %>%
  tidyr::pivot_longer(cols = -p_m1, names_to = "Metric", values_to = "Value")

# Plot the metrics as a function of p_m1
ggplot(df, aes(x = p_m1, y = Value, color = Metric)) +
  geom_line(size = 1) +  # Add lines for each metric
  labs(title = "Gini Index, Classification Error, and Entropy",
       x = expression(hat(p)[m1]), y = "Value") +
  theme_minimal()  # Use a clean theme for better visualization

```

## ISL Exercise 8.4.4 (10pts)

![](images/clipboard-3811150071.png)

------------------------------------------------------------------------

Solution:

![](images/clipboard-721214745.jpeg)

## ISL Exercise 8.4.5 (10pts)

![](images/clipboard-946799181.png)

------------------------------------------------------------------------

Solution:

Given probabilities of $P(Class\ is\ Red∣X)$: 0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7,0.75

Majority Vote Approach: Each estimate can be converted into a binary decision by using a threshold of $0.5:$ If $P(Red∣X)≥0.5,$ classify as Red. If $P(Red∣X)<0.5,$classify as Green.

Now, applying this threshold: Green $P<0.5\ :\ 4$ times.

Red $P>= 0.5 :\ 6$ times.

Since Red occurs more often, the majority vote approach classifies Red.

Average Probability Approach:

$\frac{0.1 + 0.15 + 0.2 + 0.2 + 0.55 + 0.6 + 0.6 + 0.65 + 0.7 + 0.75}{10}\ =\ 0.45$\
Since $0.45 < 0.5,$ the final classification under the average probability approach is Green.

Majority vote: Red

Average probability: Green

## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, and boosting methods for predicting `medv`. Evaluate out-of-sample performance on a test set.

------------------------------------------------------------------------

Solution:

## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. Evaluate out-of-sample performance on a test set.

------------------------------------------------------------------------

Solution:
