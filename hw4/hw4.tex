% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{caption}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{array}
\usepackage{anyfontsize}
\usepackage{multirow}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Biostat 212a Homework 4},
  pdfauthor={Wenqiang Ge UID:106371961},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Biostat 212a Homework 4}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Due Mar.~4, 2025 @ 11:59PM}
\author{Wenqiang Ge UID:106371961}
\date{2025-03-02}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, enhanced, interior hidden, breakable, sharp corners, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(rpart)}
\FunctionTok{library}\NormalTok{(rpart.plot)}
\FunctionTok{library}\NormalTok{(GGally)}
\FunctionTok{library}\NormalTok{(gtsummary)}
\FunctionTok{library}\NormalTok{(ranger)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{library}\NormalTok{(ISLR2)}
\FunctionTok{library}\NormalTok{(MASS)       }
\FunctionTok{library}\NormalTok{(randomForest) }
\FunctionTok{library}\NormalTok{(gbm)      }
\FunctionTok{library}\NormalTok{(caret)   }
\FunctionTok{library}\NormalTok{(Metrics) }
\FunctionTok{library}\NormalTok{(doParallel)}
\FunctionTok{library}\NormalTok{(future)}
\FunctionTok{library}\NormalTok{(vip)}
\FunctionTok{library}\NormalTok{(xgboost)}
\end{Highlighting}
\end{Shaded}

\hypertarget{isl-exercise-8.4.3-10pts}{%
\subsection{ISL Exercise 8.4.3 (10pts)}\label{isl-exercise-8.4.3-10pts}}

\includegraphics{images/clipboard-2954413279.png}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Solution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define probability range for class 1}
\NormalTok{p\_m1 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)  }\CommentTok{\# Probability values from 0 to 1}
\NormalTok{p\_m2 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p\_m1  }\CommentTok{\# Probability of the second class}

\CommentTok{\# Compute the three metrics}
\NormalTok{gini\_index }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (p\_m1}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ p\_m2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)  }\CommentTok{\# Gini index formula}
\NormalTok{classification\_error }\OtherTok{\textless{}{-}} \FunctionTok{pmin}\NormalTok{(p\_m1, p\_m2)  }\CommentTok{\# Classification error (minimum probability)}
\NormalTok{entropy }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{ (p\_m1 }\SpecialCharTok{*} \FunctionTok{log2}\NormalTok{(p\_m1) }\SpecialCharTok{+}\NormalTok{ p\_m2 }\SpecialCharTok{*} \FunctionTok{log2}\NormalTok{(p\_m2))  }\CommentTok{\# Entropy formula}
\NormalTok{entropy[}\FunctionTok{is.na}\NormalTok{(entropy)] }\OtherTok{\textless{}{-}} \DecValTok{0}  \CommentTok{\# Handle log(0) cases (replace NaN with 0)}

\CommentTok{\# Create a dataframe with all values}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(p\_m1, gini\_index, classification\_error, entropy) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{p\_m1, }\AttributeTok{names\_to =} \StringTok{"Metric"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Value"}\NormalTok{)}

\CommentTok{\# Plot the metrics as a function of p\_m1}
\FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ p\_m1, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{color =}\NormalTok{ Metric)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# Add lines for each metric}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Gini Index, Classification Error, and Entropy"}\NormalTok{,}
       \AttributeTok{x =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{hat}\NormalTok{(p)[m1]), }\AttributeTok{y =} \StringTok{"Value"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()  }\CommentTok{\# Use a clean theme for better visualization}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
i Please use `linewidth` instead.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\hypertarget{isl-exercise-8.4.4-10pts}{%
\subsection{ISL Exercise 8.4.4 (10pts)}\label{isl-exercise-8.4.4-10pts}}

\includegraphics{images/clipboard-3811150071.png}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Solution:

\includegraphics{images/clipboard-721214745.jpeg}

\hypertarget{isl-exercise-8.4.5-10pts}{%
\subsection{ISL Exercise 8.4.5 (10pts)}\label{isl-exercise-8.4.5-10pts}}

\includegraphics{images/clipboard-946799181.png}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Solution:

Given probabilities of \(P(Class\ is\ Red∣X)\):
0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7,0.75

Majority Vote Approach: Each estimate can be converted into a binary
decision by using a threshold of \(0.5:\) If \(P(Red∣X)≥0.5,\) classify
as Red. If \(P(Red∣X)<0.5,\)classify as Green.

Now, applying this threshold: Green \(P<0.5\ :\ 4\) times.

Red \(P>= 0.5 :\ 6\) times.

Since Red occurs more often, the majority vote approach classifies Red.

Average Probability Approach:

\(\frac{0.1 + 0.15 + 0.2 + 0.2 + 0.55 + 0.6 + 0.6 + 0.65 + 0.7 + 0.75}{10}\ =\ 0.45\)\\
Since \(0.45 < 0.5,\) the final classification under the average
probability approach is Green.

Majority vote: Red

Average probability: Green

\hypertarget{isl-lab-8.3.-boston-data-set-30pts}{%
\subsection{\texorpdfstring{ISL Lab 8.3. \texttt{Boston} data set
(30pts)}{ISL Lab 8.3. Boston data set (30pts)}}\label{isl-lab-8.3.-boston-data-set-30pts}}

Follow the machine learning workflow to train regression tree, random
forest, and boosting methods for predicting \texttt{medv}. Evaluate
out-of-sample performance on a test set.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Solution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Boston }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{tbl\_summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}
\fontsize{12.0pt}{14.4pt}\selectfont
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc}
\toprule
\textbf{Characteristic} & \textbf{N = 506}\textsuperscript{\textit{1}} \\ 
\midrule\addlinespace[2.5pt]
crim & 0.3 (0.1, 3.7) \\ 
zn & 0 (0, 13) \\ 
indus & 9.7 (5.2, 18.1) \\ 
chas & 35 (6.9\%) \\ 
nox & 0.54 (0.45, 0.62) \\ 
rm & 6.21 (5.89, 6.63) \\ 
age & 78 (45, 94) \\ 
dis & 3.21 (2.10, 5.21) \\ 
rad &  \\ 
    1 & 20 (4.0\%) \\ 
    2 & 24 (4.7\%) \\ 
    3 & 38 (7.5\%) \\ 
    4 & 110 (22\%) \\ 
    5 & 115 (23\%) \\ 
    6 & 26 (5.1\%) \\ 
    7 & 17 (3.4\%) \\ 
    8 & 24 (4.7\%) \\ 
    24 & 132 (26\%) \\ 
tax & 330 (279, 666) \\ 
ptratio & 19.05 (17.40, 20.20) \\ 
black & 391 (375, 396) \\ 
lstat & 11 (7, 17) \\ 
medv & 21 (17, 25) \\ 
\bottomrule
\end{tabular*}
\begin{minipage}{\linewidth}
\textsuperscript{\textit{1}}Median (Q1, Q3); n (\%)\\
\end{minipage}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(medv))}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-tree}{%
\subsubsection{Regression tree}\label{regression-tree}}

\hypertarget{initial-split-into-test-and-non-test-sets}{%
\subparagraph{Initial split into test and non-test
sets}\label{initial-split-into-test-and-non-test-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# For reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{203}\NormalTok{)}

\NormalTok{data\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(}
\NormalTok{  Boston, }
  \AttributeTok{prop =} \FloatTok{0.5}
\NormalTok{  )}
\NormalTok{data\_split}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Training/Testing/Total>
<253/253/506>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Boston\_other }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Boston\_other)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 253  14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Boston\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Boston\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 253  14
\end{verbatim}

\hypertarget{recipe-r}{%
\subparagraph{Recipe (R)}\label{recipe-r}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define an untrained recipe }
\NormalTok{tree\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ Boston) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_naomit}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}       
  \FunctionTok{step\_dummy}\NormalTok{(}\FunctionTok{all\_nominal\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{step\_zv}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}    
  \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{())}

\NormalTok{tree\_recipe}
\end{Highlighting}
\end{Shaded}

\hypertarget{model}{%
\subparagraph{Model}\label{model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Model}
\NormalTok{regtree\_mod }\OtherTok{\textless{}{-}} \FunctionTok{decision\_tree}\NormalTok{(}
  \AttributeTok{cost\_complexity =} \FunctionTok{tune}\NormalTok{(),}
  \AttributeTok{tree\_depth =} \FunctionTok{tune}\NormalTok{(),}
  \AttributeTok{min\_n =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{mode =} \StringTok{"regression"}\NormalTok{,}
  \AttributeTok{engine =} \StringTok{"rpart"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

\hypertarget{workflow}{%
\subparagraph{Workflow}\label{workflow}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Workflow}
\NormalTok{tree\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(tree\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(regtree\_mod)}
\NormalTok{tree\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: decision_tree()

-- Preprocessor ----------------------------------------------------------------
4 Recipe Steps

* step_naomit()
* step_dummy()
* step_zv()
* step_normalize()

-- Model -----------------------------------------------------------------------
Decision Tree Model Specification (regression)

Main Arguments:
  cost_complexity = tune()
  tree_depth = tune()
  min_n = 5

Computational engine: rpart 
\end{verbatim}

\hypertarget{tuning-grid}{%
\subparagraph{Tuning grid}\label{tuning-grid}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Tuning}
\NormalTok{tree\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{cost\_complexity}\NormalTok{(),}
                          \FunctionTok{tree\_depth}\NormalTok{(),}
                          \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation}{%
\subparagraph{Cross-validation}\label{cross-validation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Cross{-}validation}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{203}\NormalTok{)}

\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Boston\_other, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#Fit cross{-}validation}
\NormalTok{tree\_fit }\OtherTok{\textless{}{-}}\NormalTok{ tree\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ folds,}
    \AttributeTok{grid =}\NormalTok{ tree\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(yardstick}\SpecialCharTok{::}\NormalTok{rmse, yardstick}\SpecialCharTok{::}\NormalTok{rsq)}
\NormalTok{    )}
\NormalTok{tree\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 x 4
  splits           id    .metrics             .notes          
  <list>           <chr> <list>               <list>          
1 <split [202/51]> Fold1 <tibble [1,000 x 6]> <tibble [0 x 3]>
2 <split [202/51]> Fold2 <tibble [1,000 x 6]> <tibble [0 x 3]>
3 <split [202/51]> Fold3 <tibble [1,000 x 6]> <tibble [0 x 3]>
4 <split [203/50]> Fold4 <tibble [1,000 x 6]> <tibble [0 x 3]>
5 <split [203/50]> Fold5 <tibble [1,000 x 6]> <tibble [0 x 3]>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Visualize CV results}
\NormalTok{tree\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{(}\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"rmse"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tree\_depth =} \FunctionTok{as.factor}\NormalTok{(tree\_depth)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cost\_complexity, }\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{color =}\NormalTok{ tree\_depth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"cost\_complexity"}\NormalTok{, }\AttributeTok{y =} \StringTok{"CV mse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 8
   cost_complexity tree_depth .metric .estimator  mean     n std_err
             <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl>
 1        1   e-10          1 rmse    standard   7.94      5  0.442 
 2        1   e-10          1 rsq     standard   0.347     5  0.0740
 3        1.23e-10          1 rmse    standard   7.94      5  0.442 
 4        1.23e-10          1 rsq     standard   0.347     5  0.0740
 5        1.52e-10          1 rmse    standard   7.94      5  0.442 
 6        1.52e-10          1 rsq     standard   0.347     5  0.0740
 7        1.87e-10          1 rmse    standard   7.94      5  0.442 
 8        1.87e-10          1 rsq     standard   0.347     5  0.0740
 9        2.31e-10          1 rmse    standard   7.94      5  0.442 
10        2.31e-10          1 rsq     standard   0.347     5  0.0740
   .config               
   <chr>                 
 1 Preprocessor1_Model001
 2 Preprocessor1_Model001
 3 Preprocessor1_Model002
 4 Preprocessor1_Model002
 5 Preprocessor1_Model003
 6 Preprocessor1_Model003
 7 Preprocessor1_Model004
 8 Preprocessor1_Model004
 9 Preprocessor1_Model005
10 Preprocessor1_Model005
# i 990 more rows
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

\hypertarget{finalize-the-model}{%
\subparagraph{Finalize the model}\label{finalize-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 8
  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       
1        1   e-10          8 rmse    standard    4.82     5   0.726 Preprocesso~
2        1.23e-10          8 rmse    standard    4.82     5   0.726 Preprocesso~
3        1.52e-10          8 rmse    standard    4.82     5   0.726 Preprocesso~
4        1.87e-10          8 rmse    standard    4.82     5   0.726 Preprocesso~
5        2.31e-10          8 rmse    standard    4.82     5   0.726 Preprocesso~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_tree }\OtherTok{\textless{}{-}}\NormalTok{ tree\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\NormalTok{best\_tree}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  cost_complexity tree_depth .config               
            <dbl>      <int> <chr>                 
1    0.0000000001          8 Preprocessor1_Model201
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Final workflow}
\NormalTok{final\_wf }\OtherTok{\textless{}{-}}\NormalTok{ tree\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(best\_tree)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: decision_tree()

-- Preprocessor ----------------------------------------------------------------
4 Recipe Steps

* step_naomit()
* step_dummy()
* step_zv()
* step_normalize()

-- Model -----------------------------------------------------------------------
Decision Tree Model Specification (regression)

Main Arguments:
  cost_complexity = 1e-10
  tree_depth = 8
  min_n = 5

Computational engine: rpart 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the whole training set, then predict the test cases}
\NormalTok{final\_fit }\OtherTok{\textless{}{-}} 
\NormalTok{  final\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(data\_split)}
\NormalTok{final\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Resampling results
# Manual resampling 
# A tibble: 1 x 6
  splits            id               .metrics .notes   .predictions .workflow 
  <list>            <chr>            <list>   <list>   <list>       <list>    
1 <split [253/253]> train/test split <tibble> <tibble> <tibble>     <workflow>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test metrics}
\NormalTok{final\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  .metric .estimator .estimate .config             
  <chr>   <chr>          <dbl> <chr>               
1 rmse    standard       4.97  Preprocessor1_Model1
2 rsq     standard       0.716 Preprocessor1_Model1
\end{verbatim}

\hypertarget{visualize-the-final-model}{%
\subparagraph{Visualize the final
model}\label{visualize-the-final-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\OtherTok{\textless{}{-}} \FunctionTok{extract\_workflow}\NormalTok{(final\_fit)}
\NormalTok{final\_tree}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow [trained] ==========================================================
Preprocessor: Recipe
Model: decision_tree()

-- Preprocessor ----------------------------------------------------------------
4 Recipe Steps

* step_naomit()
* step_dummy()
* step_zv()
* step_normalize()

-- Model -----------------------------------------------------------------------
n= 253 

node), split, n, deviance, yval
      * denotes terminal node

  1) root 253 2.337441e+04 22.451380  
    2) rm< 0.697884 208 8.878604e+03 19.383650  
      4) lstat>=0.2888357 83 1.457504e+03 14.308430  
        8) crim>=0.16539 42 4.284764e+02 11.764290  
         16) lstat>=1.065022 22 2.211150e+02 10.150000  
           32) nox>=0.7459829 19 8.364000e+01  9.200000  
             64) lstat>=1.639685 9 1.810889e+01  7.611111  
              128) dis< -1.083781 2 8.450000e-01  5.650000 *
              129) dis>=-1.083781 7 7.374286e+00  8.171429  
                258) crim>=1.486993 3 8.000000e-02  7.200000 *
                259) crim< 1.486993 4 2.340000e+00  8.900000 *
             65) lstat< 1.639685 10 2.236100e+01 10.630000  
              130) nox< 1.203607 5 9.652000e+00  9.760000  
                260) age>=1.120657 2 4.500000e+00  8.700000 *
                261) age< 1.120657 3 1.406667e+00 10.466670 *
              131) nox>=1.203607 5 5.140000e+00 11.500000  
                262) crim>=0.9254287 2 4.500000e-02 10.650000 *
                263) crim< 0.9254287 3 2.686667e+00 12.066670 *
           33) nox< 0.7459829 3 1.172667e+01 16.166670 *
         17) lstat< 1.065022 20 8.696800e+01 13.540000  
           34) age< 1.117024 17 6.234471e+01 13.082350  
             68) rm< -0.683543 2 5.120000e+00 10.100000 *
             69) rm>=-0.683543 15 3.706400e+01 13.480000  
              138) crim>=1.111666 2 2.205000e+00 10.650000 *
              139) crim< 1.111666 13 1.637692e+01 13.915380  
                278) lstat< 0.6861861 8 2.820000e+00 13.450000 *
                279) lstat>=0.6861861 5 9.052000e+00 14.660000 *
           35) age>=1.117024 3 8.866667e-01 16.133330 *
        9) crim< 0.16539 41 4.786912e+02 16.914630  
         18) crim>=-0.4060389 37 3.045276e+02 16.291890  
           36) black< 0.2746026 16 1.295544e+02 14.731250  
             72) rad< -0.5746709 7 5.816000e+01 13.100000  
              144) age>=1.048007 2 2.178000e+01 10.300000 *
              145) age< 1.048007 5 1.442800e+01 14.220000  
                290) crim>=-0.3052468 3 1.400000e-01 13.000000 *
                291) crim< -0.3052468 2 3.125000e+00 16.050000 *
             73) rad>=-0.5746709 9 3.828000e+01 16.000000  
              146) black< -1.317335 4 7.370000e+00 14.250000 *
              147) black>=-1.317335 5 8.860000e+00 17.400000  
                294) nox>=0.7968301 3 2.906667e+00 16.533330 *
                295) nox< 0.7968301 2 3.200000e-01 18.700000 *
           37) black>=0.2746026 21 1.063124e+02 17.480950  
             74) age>=0.8609327 11 3.508727e+01 15.954550  
              148) lstat>=1.036538 4 1.347500e+00 13.925000 *
              149) lstat< 1.036538 7 7.848571e+00 17.114290  

...
and 74 more lines.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{extract\_fit\_engine}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rpart.plot}\NormalTok{(}\AttributeTok{roundint =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{extract\_fit\_parsnip}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{vip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-11-2.pdf}

}

\end{figure}

\hypertarget{random-forest}{%
\subsubsection{Random forest}\label{random-forest}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Recipe}
\NormalTok{rf\_recipe }\OtherTok{\textless{}{-}} 
  \FunctionTok{recipe}\NormalTok{(}
\NormalTok{    medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ Boston\_other}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_naomit}\NormalTok{(medv) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_zv}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{())}
\NormalTok{rf\_recipe}

\CommentTok{\#Model}
\NormalTok{rf\_mod }\OtherTok{\textless{}{-}} 
  \FunctionTok{rand\_forest}\NormalTok{(}
    \AttributeTok{mode =} \StringTok{"regression"}\NormalTok{,}
    \AttributeTok{mtry =} \FunctionTok{tune}\NormalTok{(),}
    \AttributeTok{trees =} \FunctionTok{tune}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"ranger"}\NormalTok{)}
\NormalTok{rf\_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Random Forest Model Specification (regression)

Main Arguments:
  mtry = tune()
  trees = tune()

Computational engine: ranger 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Workflow}
\NormalTok{rf\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(rf\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(rf\_mod)}
\NormalTok{rf\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: rand_forest()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_naomit()
* step_zv()

-- Model -----------------------------------------------------------------------
Random Forest Model Specification (regression)

Main Arguments:
  mtry = tune()
  trees = tune()

Computational engine: ranger 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Tuning}
\NormalTok{param\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}
  \FunctionTok{trees}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(100L, 300L)), }
  \FunctionTok{mtry}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(1L, 5L)),}
  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{param\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 15 x 2
   trees  mtry
   <int> <int>
 1   100     1
 2   200     1
 3   300     1
 4   100     2
 5   200     2
 6   300     2
 7   100     3
 8   200     3
 9   300     3
10   100     4
11   200     4
12   300     4
13   100     5
14   200     5
15   300     5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Cross{-}validation}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{203}\NormalTok{)}

\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Boston\_other, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#  5-fold cross-validation 
# A tibble: 5 x 2
  splits           id   
  <list>           <chr>
1 <split [202/51]> Fold1
2 <split [202/51]> Fold2
3 <split [202/51]> Fold3
4 <split [203/50]> Fold4
5 <split [203/50]> Fold5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_fit }\OtherTok{\textless{}{-}}\NormalTok{ rf\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ folds,}
    \AttributeTok{grid =}\NormalTok{ param\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(yardstick}\SpecialCharTok{::}\NormalTok{rmse, yardstick}\SpecialCharTok{::}\NormalTok{rsq)}
\NormalTok{    )}
\NormalTok{rf\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 x 4
  splits           id    .metrics          .notes          
  <list>           <chr> <list>            <list>          
1 <split [202/51]> Fold1 <tibble [30 x 6]> <tibble [0 x 3]>
2 <split [202/51]> Fold2 <tibble [30 x 6]> <tibble [0 x 3]>
3 <split [202/51]> Fold3 <tibble [30 x 6]> <tibble [0 x 3]>
4 <split [203/50]> Fold4 <tibble [30 x 6]> <tibble [0 x 3]>
5 <split [203/50]> Fold5 <tibble [30 x 6]> <tibble [0 x 3]>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{(}\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"rmse"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mtry =} \FunctionTok{as.factor}\NormalTok{(mtry)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ trees, }\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{color =}\NormalTok{ mtry)) }\SpecialCharTok{+}
  \CommentTok{\# geom\_point() + }
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Num. of Trees"}\NormalTok{, }\AttributeTok{y =} \StringTok{"CV mse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 8
    mtry trees .metric .estimator  mean     n std_err .config              
   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
 1     1   100 rmse    standard   5.07      5  0.417  Preprocessor1_Model01
 2     1   100 rsq     standard   0.749     5  0.0838 Preprocessor1_Model01
 3     1   200 rmse    standard   5.17      5  0.417  Preprocessor1_Model02
 4     1   200 rsq     standard   0.741     5  0.0863 Preprocessor1_Model02
 5     1   300 rmse    standard   5.13      5  0.465  Preprocessor1_Model03
 6     1   300 rsq     standard   0.741     5  0.0904 Preprocessor1_Model03
 7     2   100 rmse    standard   4.45      5  0.637  Preprocessor1_Model04
 8     2   100 rsq     standard   0.778     5  0.0988 Preprocessor1_Model04
 9     2   200 rmse    standard   4.27      5  0.564  Preprocessor1_Model05
10     2   200 rsq     standard   0.796     5  0.0859 Preprocessor1_Model05
# i 20 more rows
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 8
   mtry trees .metric .estimator  mean     n std_err .config              
  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
1     5   100 rmse    standard    3.84     5   0.612 Preprocessor1_Model13
2     5   300 rmse    standard    3.87     5   0.598 Preprocessor1_Model15
3     4   200 rmse    standard    3.87     5   0.590 Preprocessor1_Model11
4     5   200 rmse    standard    3.91     5   0.593 Preprocessor1_Model14
5     4   100 rmse    standard    3.96     5   0.602 Preprocessor1_Model10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_rf }\OtherTok{\textless{}{-}}\NormalTok{ rf\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\NormalTok{best\_rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
   mtry trees .config              
  <int> <int> <chr>                
1     5   100 Preprocessor1_Model13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Final workflow}
\NormalTok{final\_wf }\OtherTok{\textless{}{-}}\NormalTok{ rf\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(best\_rf)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: rand_forest()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_naomit()
* step_zv()

-- Model -----------------------------------------------------------------------
Random Forest Model Specification (regression)

Main Arguments:
  mtry = 5
  trees = 100

Computational engine: ranger 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the whole training set, then predict the test cases}
\NormalTok{final\_fit }\OtherTok{\textless{}{-}} 
\NormalTok{  final\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(data\_split)}
\NormalTok{final\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Resampling results
# Manual resampling 
# A tibble: 1 x 6
  splits            id               .metrics .notes   .predictions .workflow 
  <list>            <chr>            <list>   <list>   <list>       <list>    
1 <split [253/253]> train/test split <tibble> <tibble> <tibble>     <workflow>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test metrics}
\NormalTok{final\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  .metric .estimator .estimate .config             
  <chr>   <chr>          <dbl> <chr>               
1 rmse    standard       3.18  Preprocessor1_Model1
2 rsq     standard       0.869 Preprocessor1_Model1
\end{verbatim}

\hypertarget{boosting-methods}{%
\subsubsection{Boosting methods}\label{boosting-methods}}

\hypertarget{recipe-r-1}{%
\subparagraph{Recipe (R)}\label{recipe-r-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Recipe}
\NormalTok{gb\_recipe }\OtherTok{\textless{}{-}} 
  \FunctionTok{recipe}\NormalTok{(}
\NormalTok{    medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ Boston\_other}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_naomit}\NormalTok{(medv) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_zv}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{())}
\NormalTok{gb\_recipe}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-1}{%
\subparagraph{Model}\label{model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Model}
\NormalTok{gb\_mod }\OtherTok{\textless{}{-}} 
  \FunctionTok{boost\_tree}\NormalTok{(}
    \AttributeTok{mode =} \StringTok{"regression"}\NormalTok{,}
    \AttributeTok{trees =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{tree\_depth =} \FunctionTok{tune}\NormalTok{(),}
    \AttributeTok{learn\_rate =} \FunctionTok{tune}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"xgboost"}\NormalTok{)}
\NormalTok{gb\_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1000
  tree_depth = tune()
  learn_rate = tune()

Computational engine: xgboost 
\end{verbatim}

\hypertarget{workflow-tuning}{%
\subparagraph{Workflow \& Tuning}\label{workflow-tuning}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Workflow}
\NormalTok{gb\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(gb\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(gb\_mod)}
\NormalTok{gb\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: boost_tree()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_naomit()
* step_zv()

-- Model -----------------------------------------------------------------------
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1000
  tree_depth = tune()
  learn_rate = tune()

Computational engine: xgboost 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Tuning}
\NormalTok{param\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}
  \FunctionTok{tree\_depth}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(1L, 4L)),}
  \FunctionTok{learn\_rate}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{), }\AttributeTok{trans =} \FunctionTok{log10\_trans}\NormalTok{()),}
  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{  )}
\NormalTok{param\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 40 x 2
   tree_depth learn_rate
        <int>      <dbl>
 1          1    0.001  
 2          2    0.001  
 3          3    0.001  
 4          4    0.001  
 5          1    0.00190
 6          2    0.00190
 7          3    0.00190
 8          4    0.00190
 9          1    0.00359
10          2    0.00359
# i 30 more rows
\end{verbatim}

\hypertarget{cross-validation-1}{%
\subparagraph{Cross-validation}\label{cross-validation-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Cross{-}validation}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{203}\NormalTok{)}

\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Boston\_other, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#  5-fold cross-validation 
# A tibble: 5 x 2
  splits           id   
  <list>           <chr>
1 <split [202/51]> Fold1
2 <split [202/51]> Fold2
3 <split [202/51]> Fold3
4 <split [203/50]> Fold4
5 <split [203/50]> Fold5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb\_fit }\OtherTok{\textless{}{-}}\NormalTok{ gb\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ folds,}
    \AttributeTok{grid =}\NormalTok{ param\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(yardstick}\SpecialCharTok{::}\NormalTok{rmse, yardstick}\SpecialCharTok{::}\NormalTok{rsq)}
\NormalTok{    )}
\NormalTok{gb\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 x 4
  splits           id    .metrics          .notes          
  <list>           <chr> <list>            <list>          
1 <split [202/51]> Fold1 <tibble [80 x 6]> <tibble [0 x 3]>
2 <split [202/51]> Fold2 <tibble [80 x 6]> <tibble [0 x 3]>
3 <split [202/51]> Fold3 <tibble [80 x 6]> <tibble [0 x 3]>
4 <split [203/50]> Fold4 <tibble [80 x 6]> <tibble [0 x 3]>
5 <split [203/50]> Fold5 <tibble [80 x 6]> <tibble [0 x 3]>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{(}\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"rmse"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ learn\_rate, }\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{color =} \FunctionTok{factor}\NormalTok{(tree\_depth))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Learning Rate"}\NormalTok{, }\AttributeTok{y =} \StringTok{"CV AUC"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 80 x 8
   tree_depth learn_rate .metric .estimator   mean     n std_err
        <int>      <dbl> <chr>   <chr>       <dbl> <int>   <dbl>
 1          1    0.001   rmse    standard   10.7       5  0.392 
 2          1    0.001   rsq     standard    0.714     5  0.0598
 3          2    0.001   rmse    standard   10.2       5  0.445 
 4          2    0.001   rsq     standard    0.750     5  0.0861
 5          3    0.001   rmse    standard   10.2       5  0.434 
 6          3    0.001   rsq     standard    0.765     5  0.0837
 7          4    0.001   rmse    standard   10.1       5  0.418 
 8          4    0.001   rsq     standard    0.769     5  0.0820
 9          1    0.00190 rmse    standard    6.64      5  0.400 
10          1    0.00190 rsq     standard    0.752     5  0.0732
   .config              
   <chr>                
 1 Preprocessor1_Model01
 2 Preprocessor1_Model01
 3 Preprocessor1_Model02
 4 Preprocessor1_Model02
 5 Preprocessor1_Model03
 6 Preprocessor1_Model03
 7 Preprocessor1_Model04
 8 Preprocessor1_Model04
 9 Preprocessor1_Model05
10 Preprocessor1_Model05
# i 70 more rows
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/boostCV-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 8
  tree_depth learn_rate .metric .estimator  mean     n std_err .config          
       <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            
1          3     0.0880 rmse    standard    3.77     5   0.382 Preprocessor1_Mo~
2          3     0.0245 rmse    standard    3.80     5   0.422 Preprocessor1_Mo~
3          3     0.0129 rmse    standard    3.81     5   0.458 Preprocessor1_Mo~
4          2     0.167  rmse    standard    3.81     5   0.351 Preprocessor1_Mo~
5          4     0.167  rmse    standard    3.81     5   0.393 Preprocessor1_Mo~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_gb }\OtherTok{\textless{}{-}}\NormalTok{ gb\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\NormalTok{best\_gb}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  tree_depth learn_rate .config              
       <int>      <dbl> <chr>                
1          3     0.0880 Preprocessor1_Model31
\end{verbatim}

\hypertarget{finalize-the-model-1}{%
\subparagraph{Finalize the model}\label{finalize-the-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Final model}
\NormalTok{final\_wf }\OtherTok{\textless{}{-}}\NormalTok{ gb\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(best\_gb)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: boost_tree()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_naomit()
* step_zv()

-- Model -----------------------------------------------------------------------
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1000
  tree_depth = 3
  learn_rate = 0.0879922543569107

Computational engine: xgboost 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fit }\OtherTok{\textless{}{-}} 
\NormalTok{  final\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(data\_split)}
\NormalTok{final\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Resampling results
# Manual resampling 
# A tibble: 1 x 6
  splits            id               .metrics .notes   .predictions .workflow 
  <list>            <chr>            <list>   <list>   <list>       <list>    
1 <split [253/253]> train/test split <tibble> <tibble> <tibble>     <workflow>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  .metric .estimator .estimate .config             
  <chr>   <chr>          <dbl> <chr>               
1 rmse    standard       3.63  Preprocessor1_Model1
2 rsq     standard       0.836 Preprocessor1_Model1
\end{verbatim}

\hypertarget{visualize-the-final-model-1}{%
\subparagraph{Visualize the final
model}\label{visualize-the-final-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Visualize the final model}
\NormalTok{final\_tree }\OtherTok{\textless{}{-}} \FunctionTok{extract\_workflow}\NormalTok{(final\_fit)}
\NormalTok{final\_tree}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow [trained] ==========================================================
Preprocessor: Recipe
Model: boost_tree()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_naomit()
* step_zv()

-- Model -----------------------------------------------------------------------
##### xgb.Booster
raw: 1.1 Mb 
call:
  xgboost::xgb.train(params = list(eta = 0.0879922543569107, max_depth = 3L, 
    gamma = 0, colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, 
    subsample = 1), data = x$data, nrounds = 1000, watchlist = x$watchlist, 
    verbose = 0, nthread = 1, objective = "reg:squarederror")
params (as set within xgb.train):
  eta = "0.0879922543569107", max_depth = "3", gamma = "0", colsample_bytree = "1", colsample_bynode = "1", min_child_weight = "1", subsample = "1", nthread = "1", objective = "reg:squarederror", validate_parameters = "TRUE"
xgb.attributes:
  niter
callbacks:
  cb.evaluation.log()
# of features: 13 
niter: 1000
nfeatures : 13 
evaluation_log:
  iter training_rmse
 <num>         <num>
     1   21.98661584
     2   20.17570809
   ---           ---
   999    0.03930784
  1000    0.03925896
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{extract\_fit\_parsnip}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{vip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

\hypertarget{conclusion}{%
\subsubsection{Conclusion}\label{conclusion}}

Considering the values of RMSE of each model, the random forest model
has the lowest one ,so we can choose it as the final model.

\hypertarget{isl-lab-8.3-carseats-data-set-30pts}{%
\subsection{\texorpdfstring{ISL Lab 8.3 \texttt{Carseats} data set
(30pts)}{ISL Lab 8.3 Carseats data set (30pts)}}\label{isl-lab-8.3-carseats-data-set-30pts}}

Follow the machine learning workflow to train classification tree,
random forest, and boosting methods for classifying
\texttt{Sales\ \textless{}=\ 8} versus
\texttt{Sales\ \textgreater{}\ 8}. Evaluate out-of-sample performance on
a test set.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Solution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the data}
\FunctionTok{data}\NormalTok{(}\StringTok{"Carseats"}\NormalTok{, }\AttributeTok{package =} \StringTok{"ISLR"}\NormalTok{)}
\NormalTok{Carseats}\SpecialCharTok{$}\NormalTok{AHD }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Carseats}\SpecialCharTok{$}\NormalTok{Sales }\SpecialCharTok{\textgreater{}} \DecValTok{8}\NormalTok{, }\StringTok{"High"}\NormalTok{, }\StringTok{"Low"}\NormalTok{)}
\NormalTok{Carseats}\SpecialCharTok{$}\NormalTok{AHD }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Carseats}\SpecialCharTok{$}\NormalTok{AHD) }
\NormalTok{Carseats }\OtherTok{\textless{}{-}}\NormalTok{ Carseats[, }\SpecialCharTok{!}\FunctionTok{names}\NormalTok{(Carseats) }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Sales"}\NormalTok{)]}
\NormalTok{Carseats }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{tbl\_summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}
\fontsize{12.0pt}{14.4pt}\selectfont
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc}
\toprule
\textbf{Characteristic} & \textbf{N = 400}\textsuperscript{\textit{1}} \\ 
\midrule\addlinespace[2.5pt]
CompPrice & 125 (115, 135) \\ 
Income & 69 (43, 91) \\ 
Advertising & 5.0 (0.0, 12.0) \\ 
Population & 272 (139, 399) \\ 
Price & 117 (100, 131) \\ 
ShelveLoc &  \\ 
    Bad & 96 (24\%) \\ 
    Good & 85 (21\%) \\ 
    Medium & 219 (55\%) \\ 
Age & 55 (40, 66) \\ 
Education &  \\ 
    10 & 48 (12\%) \\ 
    11 & 48 (12\%) \\ 
    12 & 49 (12\%) \\ 
    13 & 43 (11\%) \\ 
    14 & 40 (10\%) \\ 
    15 & 36 (9.0\%) \\ 
    16 & 47 (12\%) \\ 
    17 & 49 (12\%) \\ 
    18 & 40 (10\%) \\ 
Urban & 282 (71\%) \\ 
US & 258 (65\%) \\ 
AHD &  \\ 
    High & 164 (41\%) \\ 
    Low & 236 (59\%) \\ 
\bottomrule
\end{tabular*}
\begin{minipage}{\linewidth}
\textsuperscript{\textit{1}}Median (Q1, Q3); n (\%)\\
\end{minipage}
\end{table}

\hypertarget{classification-tree}{%
\subsubsection{Classification tree}\label{classification-tree}}

\hypertarget{initial-split-into-test-and-non-test-sets-1}{%
\subparagraph{Initial split into test and non-test
sets}\label{initial-split-into-test-and-non-test-sets-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Initial split into test and non{-}test sets}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{212}\NormalTok{)}
\NormalTok{data\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(}
\NormalTok{  Carseats, }
  \AttributeTok{prop =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{strata =}\NormalTok{ AHD}
\NormalTok{  )}
\NormalTok{data\_split}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Training/Testing/Total>
<200/200/400>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Carseats\_other }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Carseats\_other)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 200  11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Carseats\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Carseats\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 200  11
\end{verbatim}

\hypertarget{recipe}{%
\subparagraph{Recipe}\label{recipe}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Recipe}
\NormalTok{tree\_recipe }\OtherTok{\textless{}{-}} 
  \FunctionTok{recipe}\NormalTok{(}
\NormalTok{    AHD }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ Carseats\_other}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_naomit}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# create traditional dummy variables (not necessary for random forest in R)}
  \FunctionTok{step\_dummy}\NormalTok{(}\FunctionTok{all\_nominal\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# zero{-}variance filter}
  \FunctionTok{step\_zv}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# \# center and scale numeric data (not necessary for random forest)}
  \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{()) }

\NormalTok{tree\_recipe}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-workflow}{%
\subparagraph{Model \& Workflow}\label{model-workflow}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Model}
\NormalTok{classtree\_mod }\OtherTok{\textless{}{-}} \FunctionTok{decision\_tree}\NormalTok{(}
   \CommentTok{\# Hyperparameter: Complexity parameter (cp) for pruning}
  \AttributeTok{cost\_complexity =} \FunctionTok{tune}\NormalTok{(),}
  \CommentTok{\# Hyperparameter: Maximum depth of the tree}
  \AttributeTok{tree\_depth =} \FunctionTok{tune}\NormalTok{(),}
  \AttributeTok{min\_n =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{mode =} \StringTok{"classification"}\NormalTok{,}
  \AttributeTok{engine =} \StringTok{"rpart"}
\NormalTok{  ) }

\CommentTok{\#Workflow}
\NormalTok{tree\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(tree\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(classtree\_mod) }

\CommentTok{\# Print the workflow structure}
\NormalTok{tree\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: decision_tree()

-- Preprocessor ----------------------------------------------------------------
4 Recipe Steps

* step_naomit()
* step_dummy()
* step_zv()
* step_normalize()

-- Model -----------------------------------------------------------------------
Decision Tree Model Specification (classification)

Main Arguments:
  cost_complexity = tune()
  tree_depth = tune()
  min_n = 5

Computational engine: rpart 
\end{verbatim}

\hypertarget{tuning-grid-1}{%
\subparagraph{Tuning grid}\label{tuning-grid-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Tuning}
\NormalTok{tree\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{cost\_complexity}\NormalTok{(),}
                          \FunctionTok{tree\_depth}\NormalTok{(),}
                          \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation-cv}{%
\subparagraph{Cross-validation (CV)}\label{cross-validation-cv}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{212}\NormalTok{)}

\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Carseats\_other, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#  5-fold cross-validation 
# A tibble: 5 x 2
  splits           id   
  <list>           <chr>
1 <split [160/40]> Fold1
2 <split [160/40]> Fold2
3 <split [160/40]> Fold3
4 <split [160/40]> Fold4
5 <split [160/40]> Fold5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Register a parallel backend using future}
\FunctionTok{plan}\NormalTok{(multisession, }\AttributeTok{workers =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Fit cross{-}validation.}
\NormalTok{tree\_fit }\OtherTok{\textless{}{-}}\NormalTok{ tree\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ folds,}
    \AttributeTok{grid =}\NormalTok{ tree\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(yardstick}\SpecialCharTok{::}\NormalTok{accuracy, yardstick}\SpecialCharTok{::}\NormalTok{roc\_auc),}
    \AttributeTok{control =} \FunctionTok{control\_grid}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{parallel\_over =} \StringTok{"resamples"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Stop parallel processing after computation}
\FunctionTok{plan}\NormalTok{(sequential)  }\CommentTok{\# Reset to sequential processing after tuning}

\NormalTok{tree\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 x 5
  splits           id    .metrics             .notes           .predictions
  <list>           <chr> <list>               <list>           <list>      
1 <split [160/40]> Fold1 <tibble [1,000 x 6]> <tibble [0 x 3]> <tibble>    
2 <split [160/40]> Fold2 <tibble [1,000 x 6]> <tibble [0 x 3]> <tibble>    
3 <split [160/40]> Fold3 <tibble [1,000 x 6]> <tibble [0 x 3]> <tibble>    
4 <split [160/40]> Fold4 <tibble [1,000 x 6]> <tibble [0 x 3]> <tibble>    
5 <split [160/40]> Fold5 <tibble [1,000 x 6]> <tibble [0 x 3]> <tibble>    
\end{verbatim}

\hypertarget{visualize-cv-results}{%
\subparagraph{Visualize CV results}\label{visualize-cv-results}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{(}\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"roc\_auc"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tree\_depth =} \FunctionTok{as.factor}\NormalTok{(tree\_depth)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cost\_complexity, }\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{color =}\NormalTok{ tree\_depth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"cost\_complexity"}\NormalTok{, }\AttributeTok{y =} \StringTok{"CV ROC AUC"}\NormalTok{, }\AttributeTok{color =} \StringTok{"tree\_depth"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 8
   cost_complexity tree_depth .metric  .estimator  mean     n std_err
             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl>
 1        1   e-10          1 accuracy binary     0.685     5  0.0232
 2        1   e-10          1 roc_auc  binary     0.629     5  0.0298
 3        1.23e-10          1 accuracy binary     0.685     5  0.0232
 4        1.23e-10          1 roc_auc  binary     0.629     5  0.0298
 5        1.52e-10          1 accuracy binary     0.685     5  0.0232
 6        1.52e-10          1 roc_auc  binary     0.629     5  0.0298
 7        1.87e-10          1 accuracy binary     0.685     5  0.0232
 8        1.87e-10          1 roc_auc  binary     0.629     5  0.0298
 9        2.31e-10          1 accuracy binary     0.685     5  0.0232
10        2.31e-10          1 roc_auc  binary     0.629     5  0.0298
   .config               
   <chr>                 
 1 Preprocessor1_Model001
 2 Preprocessor1_Model001
 3 Preprocessor1_Model002
 4 Preprocessor1_Model002
 5 Preprocessor1_Model003
 6 Preprocessor1_Model003
 7 Preprocessor1_Model004
 8 Preprocessor1_Model004
 9 Preprocessor1_Model005
10 Preprocessor1_Model005
# i 990 more rows
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-27-1.pdf}

}

\end{figure}

\hypertarget{finalize-the-model-2}{%
\subparagraph{Finalize the model}\label{finalize-the-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 8
  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       
1          0.0231          8 roc_auc binary     0.763     5  0.0148 Preprocesso~
2          0.0231         11 roc_auc binary     0.763     5  0.0148 Preprocesso~
3          0.0231         15 roc_auc binary     0.763     5  0.0148 Preprocesso~
4          0.0187          4 roc_auc binary     0.756     5  0.0165 Preprocesso~
5          0.0231          4 roc_auc binary     0.756     5  0.0165 Preprocesso~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the best model.}
\NormalTok{best\_tree }\OtherTok{\textless{}{-}}\NormalTok{ tree\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}
\NormalTok{best\_tree }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  cost_complexity tree_depth .config               
            <dbl>      <int> <chr>                 
1          0.0231          8 Preprocessor1_Model293
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Final workflow}
\NormalTok{final\_wf }\OtherTok{\textless{}{-}}\NormalTok{ tree\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(best\_tree)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: decision_tree()

-- Preprocessor ----------------------------------------------------------------
4 Recipe Steps

* step_naomit()
* step_dummy()
* step_zv()
* step_normalize()

-- Model -----------------------------------------------------------------------
Decision Tree Model Specification (classification)

Main Arguments:
  cost_complexity = 0.0231012970008316
  tree_depth = 8
  min_n = 5

Computational engine: rpart 
\end{verbatim}

\hypertarget{visualize-the-final-model-2}{%
\subparagraph{Visualize the final
model}\label{visualize-the-final-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the whole training set, then predict the test cases}
\NormalTok{final\_fit }\OtherTok{\textless{}{-}} 
\NormalTok{  final\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(data\_split)}
\NormalTok{final\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Resampling results
# Manual resampling 
# A tibble: 1 x 6
  splits            id               .metrics .notes   .predictions .workflow 
  <list>            <chr>            <list>   <list>   <list>       <list>    
1 <split [200/200]> train/test split <tibble> <tibble> <tibble>     <workflow>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test metrics}
\NormalTok{final\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  .metric     .estimator .estimate .config             
  <chr>       <chr>          <dbl> <chr>               
1 accuracy    binary         0.755 Preprocessor1_Model1
2 roc_auc     binary         0.760 Preprocessor1_Model1
3 brier_class binary         0.208 Preprocessor1_Model1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\OtherTok{\textless{}{-}} \FunctionTok{extract\_workflow}\NormalTok{(final\_fit)}
\NormalTok{final\_tree}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow [trained] ==========================================================
Preprocessor: Recipe
Model: decision_tree()

-- Preprocessor ----------------------------------------------------------------
4 Recipe Steps

* step_naomit()
* step_dummy()
* step_zv()
* step_normalize()

-- Model -----------------------------------------------------------------------
n= 200 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 200 82 Low (0.41000000 0.59000000)  
   2) ShelveLoc_Good>=0.6742344 44  9 High (0.79545455 0.20454545)  
     4) Price< 1.204802 36  3 High (0.91666667 0.08333333) *
     5) Price>=1.204802 8  2 Low (0.25000000 0.75000000) *
   3) ShelveLoc_Good< 0.6742344 156 47 Low (0.30128205 0.69871795)  
     6) Price< -0.9414209 26  8 High (0.69230769 0.30769231)  
      12) Income>=0.6401903 12  1 High (0.91666667 0.08333333) *
      13) Income< 0.6401903 14  7 High (0.50000000 0.50000000)  
        26) ShelveLoc_Medium>=-0.0499373 9  2 High (0.77777778 0.22222222) *
        27) ShelveLoc_Medium< -0.0499373 5  0 Low (0.00000000 1.00000000) *
     7) Price>=-0.9414209 130 29 Low (0.22307692 0.77692308)  
      14) Advertising>=0.78991 26 12 High (0.53846154 0.46153846)  
        28) Price< 0.8505711 20  6 High (0.70000000 0.30000000)  
          56) CompPrice>=-0.787038 16  2 High (0.87500000 0.12500000) *
          57) CompPrice< -0.787038 4  0 Low (0.00000000 1.00000000) *
        29) Price>=0.8505711 6  0 Low (0.00000000 1.00000000) *
      15) Advertising< 0.78991 104 15 Low (0.14423077 0.85576923)  
        30) CompPrice>=1.238547 15  6 Low (0.40000000 0.60000000)  
          60) Income>=0.247091 6  1 High (0.83333333 0.16666667) *
          61) Income< 0.247091 9  1 Low (0.11111111 0.88888889) *
        31) CompPrice< 1.238547 89  9 Low (0.10112360 0.89887640) *
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{extract\_fit\_engine}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rpart.plot}\NormalTok{(}\AttributeTok{roundint =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-29-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_tree }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{extract\_fit\_parsnip}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{vip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-29-2.pdf}

}

\end{figure}

\hypertarget{random-forest-1}{%
\subsubsection{Random forest}\label{random-forest-1}}

\hypertarget{initial-split-into-test-and-non-test-sets-2}{%
\subparagraph{Initial split into test and non-test
sets}\label{initial-split-into-test-and-non-test-sets-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Initial split into test and non{-}test sets}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{212}\NormalTok{)}

\NormalTok{data\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(}
\NormalTok{  Carseats, }
  \AttributeTok{prop =} \FloatTok{0.75}\NormalTok{,}
  \AttributeTok{strata =}\NormalTok{ AHD}
\NormalTok{  )}
\NormalTok{data\_split}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Training/Testing/Total>
<300/100/400>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Carseats\_other }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Carseats\_other)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 300  11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Carseats\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Carseats\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 100  11
\end{verbatim}

\hypertarget{recipe-r-2}{%
\subparagraph{Recipe (R)}\label{recipe-r-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Recipe}
\NormalTok{rf\_recipe }\OtherTok{\textless{}{-}} 
  \FunctionTok{recipe}\NormalTok{(}
\NormalTok{    AHD }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ Carseats\_other}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_zv}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{())}
\NormalTok{rf\_recipe}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-2}{%
\subparagraph{Model}\label{model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Model}
\NormalTok{rf\_mod }\OtherTok{\textless{}{-}} 
  \FunctionTok{rand\_forest}\NormalTok{(}
    \AttributeTok{mode =} \StringTok{"classification"}\NormalTok{,}
    \AttributeTok{mtry =} \FunctionTok{tune}\NormalTok{(),}
    \AttributeTok{trees =} \FunctionTok{tune}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"ranger"}\NormalTok{)}
\NormalTok{rf\_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Random Forest Model Specification (classification)

Main Arguments:
  mtry = tune()
  trees = tune()

Computational engine: ranger 
\end{verbatim}

\hypertarget{work-tuning}{%
\subparagraph{Work \& Tuning}\label{work-tuning}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Workflow}
\NormalTok{rf\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(rf\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(rf\_mod)}
\NormalTok{rf\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: rand_forest()

-- Preprocessor ----------------------------------------------------------------
1 Recipe Step

* step_zv()

-- Model -----------------------------------------------------------------------
Random Forest Model Specification (classification)

Main Arguments:
  mtry = tune()
  trees = tune()

Computational engine: ranger 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Tuning}
\NormalTok{param\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}
  \FunctionTok{trees}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(100L, 300L)), }
  \FunctionTok{mtry}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(1L, 5L)),}
  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{param\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 15 x 2
   trees  mtry
   <int> <int>
 1   100     1
 2   200     1
 3   300     1
 4   100     2
 5   200     2
 6   300     2
 7   100     3
 8   200     3
 9   300     3
10   100     4
11   200     4
12   300     4
13   100     5
14   200     5
15   300     5
\end{verbatim}

\hypertarget{cross-validation-cv-1}{%
\subparagraph{Cross-validation (CV)}\label{cross-validation-cv-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Cross{-}validation}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{203}\NormalTok{)}

\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Carseats\_other, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#  5-fold cross-validation 
# A tibble: 5 x 2
  splits           id   
  <list>           <chr>
1 <split [240/60]> Fold1
2 <split [240/60]> Fold2
3 <split [240/60]> Fold3
4 <split [240/60]> Fold4
5 <split [240/60]> Fold5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Fit cross{-}validation}
\NormalTok{rf\_fit }\OtherTok{\textless{}{-}}\NormalTok{ rf\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ folds,}
    \AttributeTok{grid =}\NormalTok{ param\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(yardstick}\SpecialCharTok{::}\NormalTok{roc\_auc,}
\NormalTok{                         yardstick}\SpecialCharTok{::}\NormalTok{accuracy)}
\NormalTok{    )}
\NormalTok{rf\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 x 4
  splits           id    .metrics          .notes          
  <list>           <chr> <list>            <list>          
1 <split [240/60]> Fold1 <tibble [30 x 6]> <tibble [0 x 3]>
2 <split [240/60]> Fold2 <tibble [30 x 6]> <tibble [0 x 3]>
3 <split [240/60]> Fold3 <tibble [30 x 6]> <tibble [0 x 3]>
4 <split [240/60]> Fold4 <tibble [30 x 6]> <tibble [0 x 3]>
5 <split [240/60]> Fold5 <tibble [30 x 6]> <tibble [0 x 3]>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Visualize CV results}
\NormalTok{rf\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{(}\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"roc\_auc"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mtry =} \FunctionTok{as.factor}\NormalTok{(mtry)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ trees, }\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{color =}\NormalTok{ mtry)) }\SpecialCharTok{+}
  \CommentTok{\# geom\_point() + }
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Num. of Trees"}\NormalTok{, }\AttributeTok{y =} \StringTok{"CV AUC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 8
    mtry trees .metric  .estimator  mean     n std_err .config              
   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                
 1     1   100 accuracy binary     0.753     5  0.0207 Preprocessor1_Model01
 2     1   100 roc_auc  binary     0.848     5  0.0214 Preprocessor1_Model01
 3     1   200 accuracy binary     0.747     5  0.0276 Preprocessor1_Model02
 4     1   200 roc_auc  binary     0.843     5  0.0297 Preprocessor1_Model02
 5     1   300 accuracy binary     0.76      5  0.0327 Preprocessor1_Model03
 6     1   300 roc_auc  binary     0.850     5  0.0283 Preprocessor1_Model03
 7     2   100 accuracy binary     0.767     5  0.0497 Preprocessor1_Model04
 8     2   100 roc_auc  binary     0.861     5  0.0324 Preprocessor1_Model04
 9     2   200 accuracy binary     0.777     5  0.0327 Preprocessor1_Model05
10     2   200 roc_auc  binary     0.871     5  0.0274 Preprocessor1_Model05
# i 20 more rows
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-34-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Show the top 5 models.}
\NormalTok{rf\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 8
   mtry trees .metric .estimator  mean     n std_err .config              
  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
1     5   200 roc_auc binary     0.883     5  0.0264 Preprocessor1_Model14
2     5   100 roc_auc binary     0.882     5  0.0317 Preprocessor1_Model13
3     3   300 roc_auc binary     0.881     5  0.0306 Preprocessor1_Model09
4     5   300 roc_auc binary     0.880     5  0.0322 Preprocessor1_Model15
5     4   300 roc_auc binary     0.880     5  0.0284 Preprocessor1_Model12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Select the best model}
\NormalTok{best\_rf }\OtherTok{\textless{}{-}}\NormalTok{ rf\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}
\NormalTok{best\_rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
   mtry trees .config              
  <int> <int> <chr>                
1     5   200 Preprocessor1_Model14
\end{verbatim}

\hypertarget{finalize-the-model-3}{%
\subparagraph{Finalize the model}\label{finalize-the-model-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Final model}
\NormalTok{final\_wf }\OtherTok{\textless{}{-}}\NormalTok{ rf\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(best\_rf)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: rand_forest()

-- Preprocessor ----------------------------------------------------------------
1 Recipe Step

* step_zv()

-- Model -----------------------------------------------------------------------
Random Forest Model Specification (classification)

Main Arguments:
  mtry = 5
  trees = 200

Computational engine: ranger 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fit }\OtherTok{\textless{}{-}} 
\NormalTok{  final\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(data\_split)}
\NormalTok{final\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Resampling results
# Manual resampling 
# A tibble: 1 x 6
  splits            id               .metrics .notes   .predictions .workflow 
  <list>            <chr>            <list>   <list>   <list>       <list>    
1 <split [300/100]> train/test split <tibble> <tibble> <tibble>     <workflow>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  .metric     .estimator .estimate .config             
  <chr>       <chr>          <dbl> <chr>               
1 accuracy    binary         0.81  Preprocessor1_Model1
2 roc_auc     binary         0.895 Preprocessor1_Model1
3 brier_class binary         0.140 Preprocessor1_Model1
\end{verbatim}

\hypertarget{boosting-methods-1}{%
\subsubsection{Boosting methods}\label{boosting-methods-1}}

\hypertarget{initial-split-into-test-and-non-test-sets-3}{%
\subparagraph{Initial split into test and non-test
sets}\label{initial-split-into-test-and-non-test-sets-3}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(xgboost)}
\CommentTok{\#Initial split into test and non{-}test sets}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{212}\NormalTok{)}

\NormalTok{data\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(}
\NormalTok{  Carseats, }
  \AttributeTok{prop =} \FloatTok{0.75}\NormalTok{,}
  \AttributeTok{strata =}\NormalTok{ AHD}
\NormalTok{  )}
\NormalTok{data\_split}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Training/Testing/Total>
<300/100/400>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Carseats\_other }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Carseats\_other)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 300  11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Carseats\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(data\_split)}
\FunctionTok{dim}\NormalTok{(Carseats\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 100  11
\end{verbatim}

\hypertarget{recipe-r-3}{%
\subparagraph{Recipe (R)}\label{recipe-r-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Recipe}
\NormalTok{gb\_recipe }\OtherTok{\textless{}{-}} 
  \FunctionTok{recipe}\NormalTok{(}
\NormalTok{    AHD }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ Carseats\_other}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_dummy}\NormalTok{(}\FunctionTok{all\_nominal\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_zv}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{())}
\NormalTok{gb\_recipe}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-3}{%
\subparagraph{Model}\label{model-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Model}
\NormalTok{gb\_mod }\OtherTok{\textless{}{-}} 
  \FunctionTok{boost\_tree}\NormalTok{(}
    \AttributeTok{mode =} \StringTok{"classification"}\NormalTok{,}
    \AttributeTok{trees =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{tree\_depth =} \FunctionTok{tune}\NormalTok{(),}
    \AttributeTok{learn\_rate =} \FunctionTok{tune}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"xgboost"}\NormalTok{)}
\NormalTok{gb\_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Boosted Tree Model Specification (classification)

Main Arguments:
  trees = 1000
  tree_depth = tune()
  learn_rate = tune()

Computational engine: xgboost 
\end{verbatim}

\hypertarget{workflow-tuning-1}{%
\subparagraph{Workflow \& Tuning}\label{workflow-tuning-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Workflow}
\NormalTok{gb\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(gb\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(gb\_mod)}
\NormalTok{gb\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: boost_tree()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_dummy()
* step_zv()

-- Model -----------------------------------------------------------------------
Boosted Tree Model Specification (classification)

Main Arguments:
  trees = 1000
  tree_depth = tune()
  learn_rate = tune()

Computational engine: xgboost 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Tuning}
\NormalTok{param\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}
  \FunctionTok{tree\_depth}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(1L, 3L)),}
  \FunctionTok{learn\_rate}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{trans =} \FunctionTok{log10\_trans}\NormalTok{()),}
  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{  )}
\NormalTok{param\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 2
   tree_depth learn_rate
        <int>      <dbl>
 1          1  0.00001  
 2          2  0.00001  
 3          3  0.00001  
 4          1  0.0000599
 5          2  0.0000599
 6          3  0.0000599
 7          1  0.000359 
 8          2  0.000359 
 9          3  0.000359 
10          1  0.00215  
# i 20 more rows
\end{verbatim}

\hypertarget{cross-validation-2}{%
\subparagraph{Cross-validation}\label{cross-validation-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Cross{-}validation}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{203}\NormalTok{)}

\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Carseats\_other, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#  5-fold cross-validation 
# A tibble: 5 x 2
  splits           id   
  <list>           <chr>
1 <split [240/60]> Fold1
2 <split [240/60]> Fold2
3 <split [240/60]> Fold3
4 <split [240/60]> Fold4
5 <split [240/60]> Fold5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb\_fit }\OtherTok{\textless{}{-}}\NormalTok{ gb\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ folds,}
    \AttributeTok{grid =}\NormalTok{ param\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(yardstick}\SpecialCharTok{::}\NormalTok{roc\_auc,}
\NormalTok{                         yardstick}\SpecialCharTok{::}\NormalTok{accuracy)}
\NormalTok{    )}
\NormalTok{gb\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 x 4
  splits           id    .metrics          .notes          
  <list>           <chr> <list>            <list>          
1 <split [240/60]> Fold1 <tibble [60 x 6]> <tibble [0 x 3]>
2 <split [240/60]> Fold2 <tibble [60 x 6]> <tibble [0 x 3]>
3 <split [240/60]> Fold3 <tibble [60 x 6]> <tibble [0 x 3]>
4 <split [240/60]> Fold4 <tibble [60 x 6]> <tibble [0 x 3]>
5 <split [240/60]> Fold5 <tibble [60 x 6]> <tibble [0 x 3]>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{(}\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"roc\_auc"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tree\_depth =} \FunctionTok{as.factor}\NormalTok{(tree\_depth)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ learn\_rate, }\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{color =}\NormalTok{ tree\_depth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Learning Rate"}\NormalTok{, }\AttributeTok{y =} \StringTok{"CV AUC"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 60 x 8
   tree_depth learn_rate .metric  .estimator  mean     n std_err
        <int>      <dbl> <chr>    <chr>      <dbl> <int>   <dbl>
 1          1  0.00001   accuracy binary     0.717     5  0.0183
 2          1  0.00001   roc_auc  binary     0.671     5  0.0181
 3          2  0.00001   accuracy binary     0.713     5  0.0309
 4          2  0.00001   roc_auc  binary     0.729     5  0.0203
 5          3  0.00001   accuracy binary     0.737     5  0.0295
 6          3  0.00001   roc_auc  binary     0.773     5  0.0258
 7          1  0.0000599 accuracy binary     0.717     5  0.0183
 8          1  0.0000599 roc_auc  binary     0.671     5  0.0181
 9          2  0.0000599 accuracy binary     0.743     5  0.0310
10          2  0.0000599 roc_auc  binary     0.736     5  0.0187
   .config              
   <chr>                
 1 Preprocessor1_Model01
 2 Preprocessor1_Model01
 3 Preprocessor1_Model02
 4 Preprocessor1_Model02
 5 Preprocessor1_Model03
 6 Preprocessor1_Model03
 7 Preprocessor1_Model04
 8 Preprocessor1_Model04
 9 Preprocessor1_Model05
10 Preprocessor1_Model05
# i 50 more rows
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{hw4_files/figure-pdf/unnamed-chunk-40-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 8
  tree_depth learn_rate .metric .estimator  mean     n std_err .config          
       <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            
1          1     0.0774 roc_auc binary     0.928     5  0.0138 Preprocessor1_Mo~
2          3     0.0129 roc_auc binary     0.919     5  0.0161 Preprocessor1_Mo~
3          2     0.0129 roc_auc binary     0.915     5  0.0173 Preprocessor1_Mo~
4          1     0.464  roc_auc binary     0.912     5  0.0191 Preprocessor1_Mo~
5          3     0.0774 roc_auc binary     0.908     5  0.0182 Preprocessor1_Mo~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#select the best model}
\NormalTok{best\_gb }\OtherTok{\textless{}{-}}\NormalTok{ gb\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}
\NormalTok{best\_gb}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  tree_depth learn_rate .config              
       <int>      <dbl> <chr>                
1          1     0.0774 Preprocessor1_Model16
\end{verbatim}

\hypertarget{finalize-the-model-4}{%
\subparagraph{Finalize the model}\label{finalize-the-model-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Final model}
\NormalTok{final\_wf }\OtherTok{\textless{}{-}}\NormalTok{ gb\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(best\_gb)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: boost_tree()

-- Preprocessor ----------------------------------------------------------------
2 Recipe Steps

* step_dummy()
* step_zv()

-- Model -----------------------------------------------------------------------
Boosted Tree Model Specification (classification)

Main Arguments:
  trees = 1000
  tree_depth = 1
  learn_rate = 0.0774263682681127

Computational engine: xgboost 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fit }\OtherTok{\textless{}{-}} 
\NormalTok{  final\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(data\_split)}
\NormalTok{final\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Resampling results
# Manual resampling 
# A tibble: 1 x 6
  splits            id               .metrics .notes   .predictions .workflow 
  <list>            <chr>            <list>   <list>   <list>       <list>    
1 <split [300/100]> train/test split <tibble> <tibble> <tibble>     <workflow>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  .metric     .estimator .estimate .config             
  <chr>       <chr>          <dbl> <chr>               
1 accuracy    binary         0.82  Preprocessor1_Model1
2 roc_auc     binary         0.907 Preprocessor1_Model1
3 brier_class binary         0.127 Preprocessor1_Model1
\end{verbatim}

\hypertarget{conclusion-1}{%
\subsubsection{Conclusion}\label{conclusion-1}}

We choose the boosting method as the final model for classifying Sales
in the Carseats data set, because it has the highest accuracy and
roc\_auc.



\end{document}
